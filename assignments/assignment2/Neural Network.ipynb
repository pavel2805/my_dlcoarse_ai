{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "#Y = np.array([[4,0],[-2,0.3]])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "FC_Layer = FullyConnectedLayer(3, 4)\n",
    "#print('W', FC_Layer.W.value)\n",
    "#print('B', FC_Layer.B.value)\n",
    "out=FC_Layer.forward(X)\n",
    "#print('out.shape',out.shape)\n",
    "d_out=FC_Layer.backward(out)\n",
    "assert check_layer_gradient(FC_Layer, X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "y=np.array([1,1])\n",
    "print(train_y.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train_X[0:4]\n",
    "y=train_y[0:4]\n",
    "model = TwoLayerNet(n_input = X.shape[1], n_output = 10, hidden_layer_size = 4, reg = 0)\n",
    "#print('initial fc1.W.grad ', model.fc1.W.grad)\n",
    "loss = model.compute_loss_and_gradients(X, y)\n",
    "#print('fc1.W.grad after rirst call', model.fc1.W.grad)\n",
    "check_model_gradient(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3934693517681183\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss= model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "print('loss',loss)\n",
    "#loss,output = model.compute_loss_and_gradients_no_l2(train_X[:2], train_y[:2])\n",
    "#print('loss_l2',loss)\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "model_null_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model_null_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_null_reg 2.3028187229597146\n",
      "loss_with_reg 2.3934693517681183\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "model_null_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model_null_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "print('loss_null_reg', loss)\n",
    "print('loss_with_reg',loss_with_reg)\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3028187229597146\n",
      "pred [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y [9 9 2 1 7 1 3 5 3 6 4 2 9 1 0 7 2 1 4 7 5 9 5 3 8 9 6 6 5 1]\n",
      "accurcy= 0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:10], train_y[:10])\n",
    "print('loss',loss)\n",
    "pred=model_with_reg.predict(train_X[:30])\n",
    "print('pred',pred)\n",
    "print('train_y',train_y[:30])\n",
    "\n",
    "accuracy=multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) \n",
    "print('accurcy=',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 9000\n",
      "epoch number 0\n",
      "last Loss of batch: 2.241763, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 1\n",
      "last Loss of batch: 2.217488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.208190, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 3\n",
      "last Loss of batch: 2.204452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 4\n",
      "last Loss of batch: 2.202811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 5\n",
      "last Loss of batch: 2.202013, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 6\n",
      "last Loss of batch: 2.201583, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 7\n",
      "last Loss of batch: 2.201333, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 8\n",
      "last Loss of batch: 2.201179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 9\n",
      "last Loss of batch: 2.201083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 10\n",
      "last Loss of batch: 2.201023, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 11\n",
      "last Loss of batch: 2.200986, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 12\n",
      "last Loss of batch: 2.200964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 13\n",
      "last Loss of batch: 2.200952, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 14\n",
      "last Loss of batch: 2.200946, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 15\n",
      "last Loss of batch: 2.200944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 16\n",
      "last Loss of batch: 2.200944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 17\n",
      "last Loss of batch: 2.200945, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 18\n",
      "last Loss of batch: 2.200947, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 19\n",
      "last Loss of batch: 2.200949, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), num_epochs=20, learning_rate = 1e-2)\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 9000\n",
      "epoch number 0\n",
      "last Loss of batch: 2.241763, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 1\n",
      "last Loss of batch: 2.217488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.208190, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 3\n",
      "last Loss of batch: 2.204452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 4\n",
      "last Loss of batch: 2.202811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 5\n",
      "last Loss of batch: 2.202013, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 6\n",
      "last Loss of batch: 2.201583, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 7\n",
      "last Loss of batch: 2.201333, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 8\n",
      "last Loss of batch: 2.201179, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 9\n",
      "last Loss of batch: 2.201083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 10\n",
      "last Loss of batch: 2.201023, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 11\n",
      "last Loss of batch: 2.200986, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 12\n",
      "last Loss of batch: 2.200964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 13\n",
      "last Loss of batch: 2.200952, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 14\n",
      "last Loss of batch: 2.200946, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 15\n",
      "last Loss of batch: 2.200944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 16\n",
      "last Loss of batch: 2.200944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 17\n",
      "last Loss of batch: 2.200945, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 18\n",
      "last Loss of batch: 2.200947, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 19\n",
      "last Loss of batch: 2.200949, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2836eea630>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(train_history)\n",
    "#plt.plot(val_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 9000\n",
      "epoch number 0\n",
      "last Loss of batch: 2.246777, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 1\n",
      "last Loss of batch: 2.218418, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.208471, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 3\n",
      "last Loss of batch: 2.204603, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 4\n",
      "last Loss of batch: 2.202911, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 5\n",
      "last Loss of batch: 2.202080, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 6\n",
      "last Loss of batch: 2.201628, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 7\n",
      "last Loss of batch: 2.201360, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 8\n",
      "last Loss of batch: 2.201192, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 9\n",
      "last Loss of batch: 2.201083, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 10\n",
      "last Loss of batch: 2.201011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 11\n",
      "last Loss of batch: 2.200964, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 12\n",
      "last Loss of batch: 2.200932, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 13\n",
      "last Loss of batch: 2.200912, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 14\n",
      "last Loss of batch: 2.200899, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 15\n",
      "last Loss of batch: 2.200890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 16\n",
      "last Loss of batch: 2.200884, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 17\n",
      "last Loss of batch: 2.200881, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 18\n",
      "last Loss of batch: 2.200878, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 19\n",
      "last Loss of batch: 2.200877, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 9000\n",
      "epoch number 0\n",
      "last Loss of batch: 2.319888, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 1\n",
      "last Loss of batch: 2.307618, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), num_epochs=2, learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 15\n",
      "epoch number 0\n",
      "last Loss of batch: 2.315821, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "epoch number 1\n",
      "last Loss of batch: 2.278243, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.240309, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "epoch number 3\n",
      "last Loss of batch: 2.187381, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "epoch number 4\n",
      "last Loss of batch: 2.148198, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "epoch number 5\n",
      "last Loss of batch: 2.047907, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "epoch number 6\n",
      "last Loss of batch: 1.968961, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 7\n",
      "last Loss of batch: 1.827301, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 8\n",
      "last Loss of batch: 1.767389, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "epoch number 9\n",
      "last Loss of batch: 1.613858, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 10\n",
      "last Loss of batch: 1.588767, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "epoch number 11\n",
      "last Loss of batch: 1.361337, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "epoch number 12\n",
      "last Loss of batch: 1.211243, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "epoch number 13\n",
      "last Loss of batch: 1.082787, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "epoch number 14\n",
      "last Loss of batch: 0.907232, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 15\n",
      "last Loss of batch: 0.708877, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 16\n",
      "last Loss of batch: 0.555702, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 17\n",
      "last Loss of batch: 0.428046, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 18\n",
      "last Loss of batch: 0.303236, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 19\n",
      "last Loss of batch: 0.202371, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 20\n",
      "last Loss of batch: 0.140655, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 21\n",
      "last Loss of batch: 0.106307, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 22\n",
      "last Loss of batch: 0.084551, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 23\n",
      "last Loss of batch: 0.069930, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 24\n",
      "last Loss of batch: 0.059070, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 25\n",
      "last Loss of batch: 0.051005, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 26\n",
      "last Loss of batch: 0.044686, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 27\n",
      "last Loss of batch: 0.039650, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 28\n",
      "last Loss of batch: 0.035627, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 29\n",
      "last Loss of batch: 0.032293, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 30\n",
      "last Loss of batch: 0.029517, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 31\n",
      "last Loss of batch: 0.027191, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 32\n",
      "last Loss of batch: 0.025221, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 33\n",
      "last Loss of batch: 0.023520, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 34\n",
      "last Loss of batch: 0.022010, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 35\n",
      "last Loss of batch: 0.020727, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 36\n",
      "last Loss of batch: 0.019582, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 37\n",
      "last Loss of batch: 0.018559, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 38\n",
      "last Loss of batch: 0.017639, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 39\n",
      "last Loss of batch: 0.016828, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 40\n",
      "last Loss of batch: 0.016095, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 41\n",
      "last Loss of batch: 0.015424, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 42\n",
      "last Loss of batch: 0.014817, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 43\n",
      "last Loss of batch: 0.014266, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 44\n",
      "last Loss of batch: 0.013760, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 45\n",
      "last Loss of batch: 0.013298, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 46\n",
      "last Loss of batch: 0.012864, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 47\n",
      "last Loss of batch: 0.012471, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 48\n",
      "last Loss of batch: 0.012104, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 49\n",
      "last Loss of batch: 0.011764, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 50\n",
      "last Loss of batch: 0.011448, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 51\n",
      "last Loss of batch: 0.011152, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 52\n",
      "last Loss of batch: 0.010876, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 53\n",
      "last Loss of batch: 0.010618, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 54\n",
      "last Loss of batch: 0.010374, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 55\n",
      "last Loss of batch: 0.010148, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 56\n",
      "last Loss of batch: 0.009931, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 57\n",
      "last Loss of batch: 0.009732, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 58\n",
      "last Loss of batch: 0.009540, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 59\n",
      "last Loss of batch: 0.009357, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 60\n",
      "last Loss of batch: 0.009190, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 61\n",
      "last Loss of batch: 0.009027, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 62\n",
      "last Loss of batch: 0.008874, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 63\n",
      "last Loss of batch: 0.008727, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 64\n",
      "last Loss of batch: 0.008592, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 65\n",
      "last Loss of batch: 0.008458, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 66\n",
      "last Loss of batch: 0.008333, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 67\n",
      "last Loss of batch: 0.008212, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 68\n",
      "last Loss of batch: 0.008100, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 69\n",
      "last Loss of batch: 0.007989, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 70\n",
      "last Loss of batch: 0.007885, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 71\n",
      "last Loss of batch: 0.007786, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 72\n",
      "last Loss of batch: 0.007689, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 73\n",
      "last Loss of batch: 0.007599, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 74\n",
      "last Loss of batch: 0.007509, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 75\n",
      "last Loss of batch: 0.007427, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 76\n",
      "last Loss of batch: 0.007343, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 77\n",
      "last Loss of batch: 0.007267, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 78\n",
      "last Loss of batch: 0.007191, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 79\n",
      "last Loss of batch: 0.007119, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 80\n",
      "last Loss of batch: 0.007050, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 81\n",
      "last Loss of batch: 0.006983, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 82\n",
      "last Loss of batch: 0.006919, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 83\n",
      "last Loss of batch: 0.006856, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 84\n",
      "last Loss of batch: 0.006797, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 85\n",
      "last Loss of batch: 0.006738, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 86\n",
      "last Loss of batch: 0.006683, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 87\n",
      "last Loss of batch: 0.006628, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 88\n",
      "last Loss of batch: 0.006575, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last Loss of batch: 0.006525, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 90\n",
      "last Loss of batch: 0.006476, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 91\n",
      "last Loss of batch: 0.006429, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 92\n",
      "last Loss of batch: 0.006382, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 93\n",
      "last Loss of batch: 0.006339, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 94\n",
      "last Loss of batch: 0.006294, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 95\n",
      "last Loss of batch: 0.006253, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 96\n",
      "last Loss of batch: 0.006212, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 97\n",
      "last Loss of batch: 0.006172, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 98\n",
      "last Loss of batch: 0.006135, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 99\n",
      "last Loss of batch: 0.006097, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 100\n",
      "last Loss of batch: 0.006062, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 101\n",
      "last Loss of batch: 0.006026, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 102\n",
      "last Loss of batch: 0.005992, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 103\n",
      "last Loss of batch: 0.005959, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 104\n",
      "last Loss of batch: 0.005926, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 105\n",
      "last Loss of batch: 0.005895, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 106\n",
      "last Loss of batch: 0.005864, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 107\n",
      "last Loss of batch: 0.005836, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 108\n",
      "last Loss of batch: 0.005806, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 109\n",
      "last Loss of batch: 0.005778, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 110\n",
      "last Loss of batch: 0.005750, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 111\n",
      "last Loss of batch: 0.005723, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 112\n",
      "last Loss of batch: 0.005698, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 113\n",
      "last Loss of batch: 0.005671, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 114\n",
      "last Loss of batch: 0.005647, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 115\n",
      "last Loss of batch: 0.005622, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 116\n",
      "last Loss of batch: 0.005598, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 117\n",
      "last Loss of batch: 0.005575, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 118\n",
      "last Loss of batch: 0.005553, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 119\n",
      "last Loss of batch: 0.005531, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 120\n",
      "last Loss of batch: 0.005509, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 121\n",
      "last Loss of batch: 0.005488, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 122\n",
      "last Loss of batch: 0.005467, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 123\n",
      "last Loss of batch: 0.005447, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 124\n",
      "last Loss of batch: 0.005428, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 125\n",
      "last Loss of batch: 0.005408, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 126\n",
      "last Loss of batch: 0.005390, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 127\n",
      "last Loss of batch: 0.005371, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 128\n",
      "last Loss of batch: 0.005353, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 129\n",
      "last Loss of batch: 0.005335, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 130\n",
      "last Loss of batch: 0.005318, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 131\n",
      "last Loss of batch: 0.005302, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 132\n",
      "last Loss of batch: 0.005284, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 133\n",
      "last Loss of batch: 0.005268, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 134\n",
      "last Loss of batch: 0.005252, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 135\n",
      "last Loss of batch: 0.005237, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 136\n",
      "last Loss of batch: 0.005222, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 137\n",
      "last Loss of batch: 0.005206, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 138\n",
      "last Loss of batch: 0.005192, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 139\n",
      "last Loss of batch: 0.005177, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 140\n",
      "last Loss of batch: 0.005163, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 141\n",
      "last Loss of batch: 0.005150, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 142\n",
      "last Loss of batch: 0.005136, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 143\n",
      "last Loss of batch: 0.005123, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 144\n",
      "last Loss of batch: 0.005110, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 145\n",
      "last Loss of batch: 0.005097, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 146\n",
      "last Loss of batch: 0.005085, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 147\n",
      "last Loss of batch: 0.005072, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 148\n",
      "last Loss of batch: 0.005060, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 149\n",
      "last Loss of batch: 0.005048, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "#model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1) #107\n",
    "#model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2) #50\n",
    "#model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3) #47\n",
    "#model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4) #47\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4) #47\n",
    "\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "#trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "#trainer = Trainer(model, dataset, SGD(), learning_rate=2e-1, num_epochs=150, batch_size=5) # lr=2e-1,reg=1e-4,epoch 25\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=150, batch_size=5) # lr=3e-1,reg=1e-4,epoch 18\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 15\n",
      "epoch number 0\n",
      "last Loss of batch: 2.315821, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "epoch number 1\n",
      "last Loss of batch: 2.278243, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.240309, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "epoch number 3\n",
      "last Loss of batch: 2.187381, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "epoch number 4\n",
      "last Loss of batch: 2.148198, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "epoch number 5\n",
      "last Loss of batch: 2.047907, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "epoch number 6\n",
      "last Loss of batch: 1.968961, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 7\n",
      "last Loss of batch: 1.827301, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 8\n",
      "last Loss of batch: 1.767389, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "epoch number 9\n",
      "last Loss of batch: 1.613858, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "epoch number 10\n",
      "last Loss of batch: 1.588767, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "epoch number 11\n",
      "last Loss of batch: 1.361337, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "epoch number 12\n",
      "last Loss of batch: 1.211243, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "epoch number 13\n",
      "last Loss of batch: 1.082787, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "epoch number 14\n",
      "last Loss of batch: 0.907232, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 15\n",
      "last Loss of batch: 0.708877, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 16\n",
      "last Loss of batch: 0.555702, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 17\n",
      "last Loss of batch: 0.428046, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "epoch number 18\n",
      "last Loss of batch: 0.303236, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "epoch number 19\n",
      "last Loss of batch: 0.202371, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num.train 9000\n",
      "epoch number 0\n",
      "last Loss of batch: 2.177720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 1\n",
      "last Loss of batch: 2.154649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "epoch number 2\n",
      "last Loss of batch: 2.041414, Train accuracy: 0.218333, val accuracy: 0.226000\n",
      "epoch number 3\n",
      "last Loss of batch: 1.971798, Train accuracy: 0.269333, val accuracy: 0.265000\n",
      "epoch number 4\n",
      "last Loss of batch: 1.848256, Train accuracy: 0.320111, val accuracy: 0.325000\n",
      "epoch number 5\n",
      "last Loss of batch: 1.712445, Train accuracy: 0.394333, val accuracy: 0.393000\n",
      "epoch number 6\n",
      "last Loss of batch: 1.586377, Train accuracy: 0.459889, val accuracy: 0.467000\n",
      "epoch number 7\n",
      "last Loss of batch: 1.470305, Train accuracy: 0.522333, val accuracy: 0.530000\n",
      "epoch number 8\n",
      "last Loss of batch: 1.367665, Train accuracy: 0.566556, val accuracy: 0.573000\n",
      "epoch number 9\n",
      "last Loss of batch: 1.266813, Train accuracy: 0.599889, val accuracy: 0.616000\n",
      "epoch number 10\n",
      "last Loss of batch: 1.198698, Train accuracy: 0.629444, val accuracy: 0.633000\n",
      "epoch number 11\n",
      "last Loss of batch: 1.134895, Train accuracy: 0.653000, val accuracy: 0.646000\n",
      "epoch number 12\n",
      "last Loss of batch: 1.082532, Train accuracy: 0.671667, val accuracy: 0.657000\n",
      "epoch number 13\n",
      "last Loss of batch: 1.043116, Train accuracy: 0.686778, val accuracy: 0.670000\n",
      "epoch number 14\n",
      "last Loss of batch: 1.006246, Train accuracy: 0.700889, val accuracy: 0.685000\n",
      "epoch number 15\n",
      "last Loss of batch: 0.976454, Train accuracy: 0.710000, val accuracy: 0.693000\n",
      "epoch number 16\n",
      "last Loss of batch: 0.951137, Train accuracy: 0.720556, val accuracy: 0.697000\n",
      "epoch number 17\n",
      "last Loss of batch: 0.935376, Train accuracy: 0.730778, val accuracy: 0.700000\n",
      "epoch number 18\n",
      "last Loss of batch: 0.924093, Train accuracy: 0.736667, val accuracy: 0.701000\n",
      "epoch number 19\n",
      "last Loss of batch: 0.915062, Train accuracy: 0.743444, val accuracy: 0.704000\n",
      "epoch number 20\n",
      "last Loss of batch: 0.904033, Train accuracy: 0.755333, val accuracy: 0.713000\n",
      "epoch number 21\n",
      "last Loss of batch: 0.897069, Train accuracy: 0.757222, val accuracy: 0.712000\n",
      "epoch number 22\n",
      "last Loss of batch: 0.891041, Train accuracy: 0.765111, val accuracy: 0.712000\n",
      "epoch number 23\n",
      "last Loss of batch: 0.891371, Train accuracy: 0.773556, val accuracy: 0.714000\n",
      "epoch number 24\n",
      "last Loss of batch: 0.884377, Train accuracy: 0.778778, val accuracy: 0.722000\n",
      "epoch number 25\n",
      "last Loss of batch: 0.858748, Train accuracy: 0.785889, val accuracy: 0.721000\n",
      "epoch number 26\n",
      "last Loss of batch: 0.871796, Train accuracy: 0.791111, val accuracy: 0.728000\n",
      "epoch number 27\n",
      "last Loss of batch: 0.847388, Train accuracy: 0.796444, val accuracy: 0.721000\n",
      "epoch number 28\n",
      "last Loss of batch: 0.841417, Train accuracy: 0.799444, val accuracy: 0.723000\n",
      "epoch number 29\n",
      "last Loss of batch: 0.837477, Train accuracy: 0.804778, val accuracy: 0.728000\n",
      "epoch number 30\n",
      "last Loss of batch: 0.828204, Train accuracy: 0.808444, val accuracy: 0.727000\n",
      "epoch number 31\n",
      "last Loss of batch: 0.811982, Train accuracy: 0.812111, val accuracy: 0.728000\n",
      "epoch number 32\n",
      "last Loss of batch: 0.800649, Train accuracy: 0.816667, val accuracy: 0.728000\n",
      "epoch number 33\n",
      "last Loss of batch: 0.791139, Train accuracy: 0.821889, val accuracy: 0.736000\n",
      "epoch number 34\n",
      "last Loss of batch: 0.779655, Train accuracy: 0.824889, val accuracy: 0.737000\n",
      "epoch number 35\n",
      "last Loss of batch: 0.785783, Train accuracy: 0.827111, val accuracy: 0.737000\n",
      "epoch number 36\n",
      "last Loss of batch: 0.773053, Train accuracy: 0.828222, val accuracy: 0.736000\n",
      "epoch number 37\n",
      "last Loss of batch: 0.764989, Train accuracy: 0.832667, val accuracy: 0.733000\n",
      "epoch number 38\n",
      "last Loss of batch: 0.764326, Train accuracy: 0.837778, val accuracy: 0.740000\n",
      "epoch number 39\n",
      "last Loss of batch: 0.762234, Train accuracy: 0.839556, val accuracy: 0.743000\n",
      "epoch number 40\n",
      "last Loss of batch: 0.750557, Train accuracy: 0.843000, val accuracy: 0.740000\n",
      "epoch number 41\n",
      "last Loss of batch: 0.739480, Train accuracy: 0.845222, val accuracy: 0.738000\n",
      "epoch number 42\n",
      "last Loss of batch: 0.734784, Train accuracy: 0.848889, val accuracy: 0.742000\n",
      "epoch number 43\n",
      "last Loss of batch: 0.738698, Train accuracy: 0.851222, val accuracy: 0.746000\n",
      "epoch number 44\n",
      "last Loss of batch: 0.724495, Train accuracy: 0.852667, val accuracy: 0.743000\n",
      "epoch number 45\n",
      "last Loss of batch: 0.723918, Train accuracy: 0.855889, val accuracy: 0.746000\n",
      "epoch number 46\n",
      "last Loss of batch: 0.715974, Train accuracy: 0.855444, val accuracy: 0.739000\n",
      "epoch number 47\n",
      "last Loss of batch: 0.718517, Train accuracy: 0.860556, val accuracy: 0.747000\n",
      "epoch number 48\n",
      "last Loss of batch: 0.715050, Train accuracy: 0.864667, val accuracy: 0.753000\n",
      "epoch number 49\n",
      "last Loss of batch: 0.717941, Train accuracy: 0.866444, val accuracy: 0.752000\n",
      "epoch number 50\n",
      "last Loss of batch: 0.713602, Train accuracy: 0.867333, val accuracy: 0.750000\n",
      "epoch number 51\n",
      "last Loss of batch: 0.703180, Train accuracy: 0.869333, val accuracy: 0.752000\n",
      "epoch number 52\n",
      "last Loss of batch: 0.700513, Train accuracy: 0.871111, val accuracy: 0.752000\n",
      "epoch number 53\n",
      "last Loss of batch: 0.693091, Train accuracy: 0.871444, val accuracy: 0.751000\n",
      "epoch number 54\n",
      "last Loss of batch: 0.708893, Train accuracy: 0.872444, val accuracy: 0.753000\n",
      "epoch number 55\n",
      "last Loss of batch: 0.689722, Train accuracy: 0.875111, val accuracy: 0.754000\n",
      "epoch number 56\n",
      "last Loss of batch: 0.705037, Train accuracy: 0.877667, val accuracy: 0.753000\n",
      "epoch number 57\n",
      "last Loss of batch: 0.703870, Train accuracy: 0.876667, val accuracy: 0.754000\n",
      "epoch number 58\n",
      "last Loss of batch: 0.687922, Train accuracy: 0.878333, val accuracy: 0.755000\n",
      "epoch number 59\n",
      "last Loss of batch: 0.698944, Train accuracy: 0.880556, val accuracy: 0.754000\n",
      "epoch number 60\n",
      "last Loss of batch: 0.699656, Train accuracy: 0.881222, val accuracy: 0.758000\n",
      "epoch number 61\n",
      "last Loss of batch: 0.703378, Train accuracy: 0.883667, val accuracy: 0.753000\n",
      "epoch number 62\n",
      "last Loss of batch: 0.698892, Train accuracy: 0.886556, val accuracy: 0.754000\n",
      "epoch number 63\n",
      "last Loss of batch: 0.690594, Train accuracy: 0.886778, val accuracy: 0.751000\n",
      "epoch number 64\n",
      "last Loss of batch: 0.688739, Train accuracy: 0.887778, val accuracy: 0.755000\n",
      "epoch number 65\n",
      "last Loss of batch: 0.679202, Train accuracy: 0.888889, val accuracy: 0.756000\n",
      "epoch number 66\n",
      "last Loss of batch: 0.678467, Train accuracy: 0.889222, val accuracy: 0.757000\n",
      "epoch number 67\n",
      "last Loss of batch: 0.684189, Train accuracy: 0.889889, val accuracy: 0.755000\n",
      "epoch number 68\n",
      "last Loss of batch: 0.674307, Train accuracy: 0.892333, val accuracy: 0.757000\n",
      "epoch number 69\n",
      "last Loss of batch: 0.677342, Train accuracy: 0.890000, val accuracy: 0.754000\n",
      "epoch number 70\n",
      "last Loss of batch: 0.684616, Train accuracy: 0.892444, val accuracy: 0.753000\n",
      "epoch number 71\n",
      "last Loss of batch: 0.679983, Train accuracy: 0.892667, val accuracy: 0.755000\n",
      "epoch number 72\n",
      "last Loss of batch: 0.681109, Train accuracy: 0.894667, val accuracy: 0.756000\n",
      "epoch number 73\n",
      "last Loss of batch: 0.690109, Train accuracy: 0.895778, val accuracy: 0.755000\n",
      "epoch number 74\n",
      "last Loss of batch: 0.684954, Train accuracy: 0.895000, val accuracy: 0.757000\n",
      "epoch number 75\n",
      "last Loss of batch: 0.680756, Train accuracy: 0.897556, val accuracy: 0.752000\n",
      "epoch number 76\n",
      "last Loss of batch: 0.678641, Train accuracy: 0.899111, val accuracy: 0.754000\n",
      "epoch number 77\n",
      "last Loss of batch: 0.700249, Train accuracy: 0.897333, val accuracy: 0.756000\n",
      "epoch number 78\n",
      "last Loss of batch: 0.680334, Train accuracy: 0.899556, val accuracy: 0.757000\n",
      "epoch number 79\n",
      "last Loss of batch: 0.682938, Train accuracy: 0.899556, val accuracy: 0.757000\n",
      "epoch number 80\n",
      "last Loss of batch: 0.678117, Train accuracy: 0.899444, val accuracy: 0.759000\n",
      "epoch number 81\n",
      "last Loss of batch: 0.678566, Train accuracy: 0.899000, val accuracy: 0.754000\n",
      "epoch number 82\n",
      "last Loss of batch: 0.676993, Train accuracy: 0.900444, val accuracy: 0.755000\n",
      "epoch number 83\n",
      "last Loss of batch: 0.673455, Train accuracy: 0.900667, val accuracy: 0.753000\n",
      "epoch number 84\n",
      "last Loss of batch: 0.669279, Train accuracy: 0.902333, val accuracy: 0.756000\n",
      "epoch number 85\n",
      "last Loss of batch: 0.668758, Train accuracy: 0.904778, val accuracy: 0.763000\n",
      "epoch number 86\n",
      "last Loss of batch: 0.673130, Train accuracy: 0.903778, val accuracy: 0.756000\n",
      "epoch number 87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last Loss of batch: 0.667269, Train accuracy: 0.905778, val accuracy: 0.763000\n",
      "epoch number 88\n",
      "last Loss of batch: 0.677174, Train accuracy: 0.904333, val accuracy: 0.753000\n",
      "epoch number 89\n",
      "last Loss of batch: 0.667839, Train accuracy: 0.907000, val accuracy: 0.757000\n",
      "epoch number 90\n",
      "last Loss of batch: 0.668168, Train accuracy: 0.905889, val accuracy: 0.761000\n",
      "epoch number 91\n",
      "last Loss of batch: 0.670115, Train accuracy: 0.905778, val accuracy: 0.755000\n",
      "epoch number 92\n",
      "last Loss of batch: 0.672485, Train accuracy: 0.904556, val accuracy: 0.758000\n",
      "epoch number 93\n",
      "last Loss of batch: 0.666128, Train accuracy: 0.907111, val accuracy: 0.754000\n",
      "epoch number 94\n",
      "last Loss of batch: 0.667279, Train accuracy: 0.907778, val accuracy: 0.759000\n",
      "epoch number 95\n",
      "last Loss of batch: 0.661940, Train accuracy: 0.908556, val accuracy: 0.759000\n",
      "epoch number 96\n",
      "last Loss of batch: 0.666335, Train accuracy: 0.908111, val accuracy: 0.761000\n",
      "epoch number 97\n",
      "last Loss of batch: 0.664371, Train accuracy: 0.910667, val accuracy: 0.758000\n",
      "epoch number 98\n",
      "last Loss of batch: 0.668264, Train accuracy: 0.909111, val accuracy: 0.753000\n",
      "epoch number 99\n",
      "last Loss of batch: 0.679573, Train accuracy: 0.912556, val accuracy: 0.764000\n",
      "epoch number 100\n",
      "last Loss of batch: 0.658319, Train accuracy: 0.911889, val accuracy: 0.764000\n",
      "epoch number 101\n",
      "last Loss of batch: 0.667521, Train accuracy: 0.911333, val accuracy: 0.764000\n",
      "epoch number 102\n",
      "last Loss of batch: 0.659817, Train accuracy: 0.912333, val accuracy: 0.758000\n",
      "epoch number 103\n",
      "last Loss of batch: 0.660797, Train accuracy: 0.911778, val accuracy: 0.760000\n",
      "epoch number 104\n",
      "last Loss of batch: 0.663011, Train accuracy: 0.913111, val accuracy: 0.766000\n",
      "epoch number 105\n",
      "last Loss of batch: 0.664780, Train accuracy: 0.912111, val accuracy: 0.765000\n",
      "epoch number 106\n",
      "last Loss of batch: 0.661179, Train accuracy: 0.914444, val accuracy: 0.764000\n",
      "epoch number 107\n",
      "last Loss of batch: 0.662556, Train accuracy: 0.913000, val accuracy: 0.765000\n",
      "epoch number 108\n",
      "last Loss of batch: 0.667776, Train accuracy: 0.913333, val accuracy: 0.765000\n",
      "epoch number 109\n",
      "last Loss of batch: 0.682617, Train accuracy: 0.913333, val accuracy: 0.764000\n",
      "epoch number 110\n",
      "last Loss of batch: 0.665054, Train accuracy: 0.915889, val accuracy: 0.764000\n",
      "epoch number 111\n",
      "last Loss of batch: 0.659095, Train accuracy: 0.914778, val accuracy: 0.762000\n",
      "epoch number 112\n",
      "last Loss of batch: 0.663737, Train accuracy: 0.916111, val accuracy: 0.768000\n",
      "epoch number 113\n",
      "last Loss of batch: 0.662238, Train accuracy: 0.915667, val accuracy: 0.769000\n",
      "epoch number 114\n",
      "last Loss of batch: 0.664838, Train accuracy: 0.915556, val accuracy: 0.768000\n",
      "epoch number 115\n",
      "last Loss of batch: 0.668294, Train accuracy: 0.915444, val accuracy: 0.769000\n",
      "epoch number 116\n",
      "last Loss of batch: 0.671285, Train accuracy: 0.916000, val accuracy: 0.765000\n",
      "epoch number 117\n",
      "last Loss of batch: 0.662790, Train accuracy: 0.918444, val accuracy: 0.767000\n",
      "epoch number 118\n",
      "last Loss of batch: 0.664474, Train accuracy: 0.916667, val accuracy: 0.772000\n",
      "epoch number 119\n",
      "last Loss of batch: 0.664254, Train accuracy: 0.916222, val accuracy: 0.767000\n",
      "epoch number 120\n",
      "last Loss of batch: 0.669101, Train accuracy: 0.916889, val accuracy: 0.768000\n",
      "epoch number 121\n",
      "last Loss of batch: 0.663380, Train accuracy: 0.919000, val accuracy: 0.770000\n",
      "epoch number 122\n",
      "last Loss of batch: 0.660946, Train accuracy: 0.917667, val accuracy: 0.760000\n",
      "epoch number 123\n",
      "last Loss of batch: 0.671031, Train accuracy: 0.918000, val accuracy: 0.765000\n",
      "epoch number 124\n",
      "last Loss of batch: 0.659717, Train accuracy: 0.918556, val accuracy: 0.768000\n",
      "epoch number 125\n",
      "last Loss of batch: 0.666050, Train accuracy: 0.917222, val accuracy: 0.766000\n",
      "epoch number 126\n",
      "last Loss of batch: 0.666287, Train accuracy: 0.917778, val accuracy: 0.768000\n",
      "epoch number 127\n",
      "last Loss of batch: 0.663555, Train accuracy: 0.918111, val accuracy: 0.769000\n",
      "epoch number 128\n",
      "last Loss of batch: 0.660652, Train accuracy: 0.918000, val accuracy: 0.766000\n",
      "epoch number 129\n",
      "last Loss of batch: 0.663761, Train accuracy: 0.918444, val accuracy: 0.769000\n",
      "epoch number 130\n",
      "last Loss of batch: 0.666482, Train accuracy: 0.918333, val accuracy: 0.768000\n",
      "epoch number 131\n",
      "last Loss of batch: 0.661486, Train accuracy: 0.919444, val accuracy: 0.774000\n",
      "epoch number 132\n",
      "last Loss of batch: 0.655769, Train accuracy: 0.918889, val accuracy: 0.768000\n",
      "epoch number 133\n",
      "last Loss of batch: 0.658520, Train accuracy: 0.916667, val accuracy: 0.769000\n",
      "epoch number 134\n",
      "last Loss of batch: 0.661324, Train accuracy: 0.919222, val accuracy: 0.769000\n",
      "epoch number 135\n",
      "last Loss of batch: 0.665805, Train accuracy: 0.918333, val accuracy: 0.773000\n",
      "epoch number 136\n",
      "last Loss of batch: 0.655743, Train accuracy: 0.917889, val accuracy: 0.769000\n",
      "epoch number 137\n",
      "last Loss of batch: 0.656637, Train accuracy: 0.919556, val accuracy: 0.768000\n",
      "epoch number 138\n",
      "last Loss of batch: 0.665925, Train accuracy: 0.919333, val accuracy: 0.770000\n",
      "epoch number 139\n",
      "last Loss of batch: 0.666081, Train accuracy: 0.922333, val accuracy: 0.765000\n",
      "epoch number 140\n",
      "last Loss of batch: 0.667907, Train accuracy: 0.921000, val accuracy: 0.768000\n",
      "epoch number 141\n",
      "last Loss of batch: 0.680514, Train accuracy: 0.916778, val accuracy: 0.765000\n",
      "epoch number 142\n",
      "last Loss of batch: 0.666014, Train accuracy: 0.920778, val accuracy: 0.766000\n",
      "epoch number 143\n",
      "last Loss of batch: 0.666487, Train accuracy: 0.920444, val accuracy: 0.769000\n",
      "epoch number 144\n",
      "last Loss of batch: 0.663805, Train accuracy: 0.921778, val accuracy: 0.766000\n",
      "epoch number 145\n",
      "last Loss of batch: 0.665982, Train accuracy: 0.920778, val accuracy: 0.768000\n",
      "epoch number 146\n",
      "last Loss of batch: 0.665579, Train accuracy: 0.922556, val accuracy: 0.770000\n",
      "epoch number 147\n",
      "last Loss of batch: 0.660935, Train accuracy: 0.921222, val accuracy: 0.769000\n",
      "epoch number 148\n",
      "last Loss of batch: 0.667288, Train accuracy: 0.922222, val accuracy: 0.766000\n",
      "epoch number 149\n",
      "last Loss of batch: 0.667292, Train accuracy: 0.921333, val accuracy: 0.769000\n",
      "epoch number 150\n",
      "last Loss of batch: 0.661207, Train accuracy: 0.922111, val accuracy: 0.773000\n",
      "epoch number 151\n",
      "last Loss of batch: 0.662113, Train accuracy: 0.922778, val accuracy: 0.770000\n",
      "epoch number 152\n",
      "last Loss of batch: 0.668130, Train accuracy: 0.922000, val accuracy: 0.768000\n",
      "epoch number 153\n",
      "last Loss of batch: 0.664989, Train accuracy: 0.921778, val accuracy: 0.770000\n",
      "epoch number 154\n",
      "last Loss of batch: 0.665950, Train accuracy: 0.922444, val accuracy: 0.767000\n",
      "epoch number 155\n",
      "last Loss of batch: 0.669829, Train accuracy: 0.920000, val accuracy: 0.762000\n",
      "epoch number 156\n",
      "last Loss of batch: 0.661917, Train accuracy: 0.922222, val accuracy: 0.766000\n",
      "epoch number 157\n",
      "last Loss of batch: 0.665313, Train accuracy: 0.924000, val accuracy: 0.766000\n",
      "epoch number 158\n",
      "last Loss of batch: 0.662947, Train accuracy: 0.923000, val accuracy: 0.769000\n",
      "epoch number 159\n",
      "last Loss of batch: 0.664967, Train accuracy: 0.922778, val accuracy: 0.766000\n",
      "epoch number 160\n",
      "last Loss of batch: 0.664246, Train accuracy: 0.922444, val accuracy: 0.765000\n",
      "epoch number 161\n",
      "last Loss of batch: 0.660581, Train accuracy: 0.921444, val accuracy: 0.766000\n",
      "epoch number 162\n",
      "last Loss of batch: 0.660645, Train accuracy: 0.924333, val accuracy: 0.771000\n",
      "epoch number 163\n",
      "last Loss of batch: 0.665486, Train accuracy: 0.924000, val accuracy: 0.766000\n",
      "epoch number 164\n",
      "last Loss of batch: 0.661984, Train accuracy: 0.924000, val accuracy: 0.765000\n",
      "epoch number 165\n",
      "last Loss of batch: 0.657339, Train accuracy: 0.924333, val accuracy: 0.765000\n",
      "epoch number 166\n",
      "last Loss of batch: 0.656213, Train accuracy: 0.923444, val accuracy: 0.765000\n",
      "epoch number 167\n",
      "last Loss of batch: 0.657518, Train accuracy: 0.924778, val accuracy: 0.768000\n",
      "epoch number 168\n",
      "last Loss of batch: 0.660389, Train accuracy: 0.924667, val accuracy: 0.763000\n",
      "epoch number 169\n",
      "last Loss of batch: 0.660621, Train accuracy: 0.924667, val accuracy: 0.767000\n",
      "epoch number 170\n",
      "last Loss of batch: 0.662190, Train accuracy: 0.925000, val accuracy: 0.769000\n",
      "epoch number 171\n",
      "last Loss of batch: 0.657491, Train accuracy: 0.926444, val accuracy: 0.772000\n",
      "epoch number 172\n",
      "last Loss of batch: 0.655752, Train accuracy: 0.925889, val accuracy: 0.766000\n",
      "epoch number 173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last Loss of batch: 0.657623, Train accuracy: 0.926889, val accuracy: 0.765000\n",
      "epoch number 174\n",
      "last Loss of batch: 0.655087, Train accuracy: 0.927556, val accuracy: 0.770000\n",
      "epoch number 175\n",
      "last Loss of batch: 0.659257, Train accuracy: 0.927444, val accuracy: 0.767000\n",
      "epoch number 176\n",
      "last Loss of batch: 0.656372, Train accuracy: 0.926333, val accuracy: 0.765000\n",
      "epoch number 177\n",
      "last Loss of batch: 0.656338, Train accuracy: 0.925444, val accuracy: 0.768000\n",
      "epoch number 178\n",
      "last Loss of batch: 0.657605, Train accuracy: 0.927000, val accuracy: 0.766000\n",
      "epoch number 179\n",
      "last Loss of batch: 0.658982, Train accuracy: 0.927111, val accuracy: 0.764000\n",
      "epoch number 180\n",
      "last Loss of batch: 0.659434, Train accuracy: 0.927000, val accuracy: 0.768000\n",
      "epoch number 181\n",
      "last Loss of batch: 0.662325, Train accuracy: 0.926778, val accuracy: 0.769000\n",
      "epoch number 182\n",
      "last Loss of batch: 0.658650, Train accuracy: 0.926000, val accuracy: 0.767000\n",
      "epoch number 183\n",
      "last Loss of batch: 0.658056, Train accuracy: 0.926333, val accuracy: 0.769000\n",
      "epoch number 184\n",
      "last Loss of batch: 0.656964, Train accuracy: 0.927444, val accuracy: 0.772000\n",
      "epoch number 185\n",
      "last Loss of batch: 0.656958, Train accuracy: 0.926556, val accuracy: 0.769000\n",
      "epoch number 186\n",
      "last Loss of batch: 0.662723, Train accuracy: 0.926111, val accuracy: 0.766000\n",
      "epoch number 187\n",
      "last Loss of batch: 0.656190, Train accuracy: 0.927444, val accuracy: 0.770000\n",
      "epoch number 188\n",
      "last Loss of batch: 0.658745, Train accuracy: 0.927000, val accuracy: 0.765000\n",
      "epoch number 189\n",
      "last Loss of batch: 0.661715, Train accuracy: 0.927111, val accuracy: 0.764000\n",
      "epoch number 190\n",
      "last Loss of batch: 0.658973, Train accuracy: 0.927000, val accuracy: 0.766000\n",
      "epoch number 191\n",
      "last Loss of batch: 0.652403, Train accuracy: 0.928556, val accuracy: 0.769000\n",
      "epoch number 192\n",
      "last Loss of batch: 0.659634, Train accuracy: 0.924222, val accuracy: 0.765000\n",
      "epoch number 193\n",
      "last Loss of batch: 0.657278, Train accuracy: 0.924222, val accuracy: 0.761000\n",
      "epoch number 194\n",
      "last Loss of batch: 0.656978, Train accuracy: 0.926444, val accuracy: 0.768000\n",
      "epoch number 195\n",
      "last Loss of batch: 0.658601, Train accuracy: 0.927556, val accuracy: 0.769000\n",
      "epoch number 196\n",
      "last Loss of batch: 0.661723, Train accuracy: 0.925444, val accuracy: 0.762000\n",
      "epoch number 197\n",
      "last Loss of batch: 0.657567, Train accuracy: 0.928667, val accuracy: 0.767000\n",
      "epoch number 198\n",
      "last Loss of batch: 0.658532, Train accuracy: 0.927222, val accuracy: 0.760000\n",
      "epoch number 199\n",
      "last Loss of batch: 0.660607, Train accuracy: 0.927333, val accuracy: 0.771000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "''' initial parameter \n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "'''\n",
    "learning_rates = 1e-1\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200 \n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=learning_rates, num_epochs = num_epochs, batch_size = batch_size)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2836e0a320>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.748000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_classifier = model\n",
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
