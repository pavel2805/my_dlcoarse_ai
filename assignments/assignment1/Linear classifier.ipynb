{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape initial (10000, 32, 32, 3)\n",
      "train_x.mean 0.446616470588233\n",
      "train_X.shape (9000, 3073)\n",
      "train_y.shape (9000,)\n",
      "val_X.shape (1000, 3073)\n",
      "val_y.shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    print('train_x.mean',mean_image[0])\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    #print('train_flat_ones.shape', train_flat_with_ones.shape)\n",
    "    #print('train_flat_ones[0]', train_flat_with_ones[0])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)\n",
    "'''\n",
    "train_X_t, train_y_t, test_X, test_y = load_svhn(\"data\", max_train=11000, max_test=1000)\n",
    "\n",
    "train_X = train_X_t[0:10000]\n",
    "train_y =train_y_t[0:10000]\n",
    "test_X = train_X_t[10000:11000]\n",
    "test_y = train_y_t[10000:11000]    \n",
    "'''\n",
    "#train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n",
    "\n",
    "print('train_X.shape initial', train_X.shape) #expect 1000,32,32,3\n",
    "#print('train_X[0,0]', train_X[0,0])\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)\n",
    "print('train_X.shape', train_X.shape)\n",
    "print('train_y.shape',train_y.shape)\n",
    "print('val_X.shape',val_X.shape)\n",
    "print('val_y.shape',val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it.shape= (1,)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it.shape= (2,)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_sum(x):\n",
    "    #print('x.shape',x.shape)\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it.shape= (1,)\n",
      "Gradient check passed!\n",
      "it.shape= (2,)\n",
      "Gradient check passed!\n",
      "it.shape= (2, 2)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "probs = linear_classifer.softmax(np.array([1, 1, 1]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0][0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "1\n",
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.zeros(5)\n",
    "print(a)\n",
    "print(np.ndim(a))\n",
    "b=a.reshape(1,-1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs [[0.57611688 0.21194156 0.21194156]]\n",
      "loss= [1.55144471]\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "print('probs',probs)\n",
    "#linear_classifer.cross_entropy_loss(probs, 1)\n",
    "loss=linear_classifer.cross_entropy_loss(probs, np.array([[1]]))\n",
    "print('loss=',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 1.551444713932051\n",
      "grad [ 0.57611688 -0.78805844  0.21194156]\n",
      "it.shape= (3,)\n",
      "Gradient check passed!\n",
      "----------star check batch------\n",
      "loss= 1.051444713932051\n",
      "grad [[ 0.28805844 -0.39402922  0.10597078]\n",
      " [-0.21194156  0.10597078  0.10597078]]\n",
      "it.shape= (2, 3)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n",
    "\n",
    "print('----------star check batch------')\n",
    "predictions=np.array([[1,0,0],[1,0,0]],dtype=np.float)\n",
    "target_index=np.array([[1],[0]],dtype=np.int)\n",
    "loss, grad =linear_classifer.softmax_with_cross_entropy_batch(predictions, target_index)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, np.array([1,0]), np.array([[1,0,0],[1,0,0]],np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- batch 2, num 4 ----------\n",
      "predictions [[ 1.  2. -1.  1.]\n",
      " [ 1.  2. -1. -1.]]\n",
      "it.shape= (2, 4)\n",
      "Gradient check passed!\n",
      "----------- batch 3, num 4 ----------\n",
      "it.shape= (3, 4)\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "\n",
    "print('----------- batch 2, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 2\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "print('----------- batch 3, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.5472365148987376\n",
      "dW= [[-0.22019927  0.22019927]\n",
      " [-0.20332316  0.20332316]\n",
      " [ 0.23874859 -0.23874859]]\n",
      "it.shape= (3, 2)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 4\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss',loss)\n",
    "print('dW=',dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it.shape= (3, 2)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index [1 1 1 1 1]\n",
      "loss SoftMax 0.44141919750255204\n",
      "dW Softmax= [[-0.16896493  0.16896493]\n",
      " [-0.15906129  0.15906129]\n",
      " [ 0.19099887 -0.19099887]]\n",
      "it.shape= (3, 2)\n",
      "Gradient check passed!\n",
      "loss L2 0.12\n",
      "dW  L2= [[ 0.02  0.04]\n",
      " [-0.02  0.02]\n",
      " [ 0.02  0.04]]\n",
      "it.shape= (3, 2)\n",
      "Gradient check passed!\n",
      "it.shape= (3, 2)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# this is my test\n",
    "batch_size = 5\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print('target_index',target_index)\n",
    "\n",
    "loss_sm, dW_sm = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss SoftMax',loss_sm)\n",
    "print('dW Softmax=',dW_sm)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "loss_l2, dW_l2=linear_classifer.l2_regularization(W, 0.01)\n",
    "print('loss L2',loss_l2)\n",
    "print('dW  L2=',dW_l2)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n",
    "\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, 0.01, X, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, 0.01,X,target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first W = none\n",
      "num_train 9000\n",
      "num_features 3073\n",
      "num_classe 10\n",
      "batch_size 20\n",
      "sections [  20   40   60   80  100  120  140  160  180  200  220  240  260  280\n",
      "  300  320  340  360  380  400  420  440  460  480  500  520  540  560\n",
      "  580  600  620  640  660  680  700  720  740  760  780  800  820  840\n",
      "  860  880  900  920  940  960  980 1000 1020 1040 1060 1080 1100 1120\n",
      " 1140 1160 1180 1200 1220 1240 1260 1280 1300 1320 1340 1360 1380 1400\n",
      " 1420 1440 1460 1480 1500 1520 1540 1560 1580 1600 1620 1640 1660 1680\n",
      " 1700 1720 1740 1760 1780 1800 1820 1840 1860 1880 1900 1920 1940 1960\n",
      " 1980 2000 2020 2040 2060 2080 2100 2120 2140 2160 2180 2200 2220 2240\n",
      " 2260 2280 2300 2320 2340 2360 2380 2400 2420 2440 2460 2480 2500 2520\n",
      " 2540 2560 2580 2600 2620 2640 2660 2680 2700 2720 2740 2760 2780 2800\n",
      " 2820 2840 2860 2880 2900 2920 2940 2960 2980 3000 3020 3040 3060 3080\n",
      " 3100 3120 3140 3160 3180 3200 3220 3240 3260 3280 3300 3320 3340 3360\n",
      " 3380 3400 3420 3440 3460 3480 3500 3520 3540 3560 3580 3600 3620 3640\n",
      " 3660 3680 3700 3720 3740 3760 3780 3800 3820 3840 3860 3880 3900 3920\n",
      " 3940 3960 3980 4000 4020 4040 4060 4080 4100 4120 4140 4160 4180 4200\n",
      " 4220 4240 4260 4280 4300 4320 4340 4360 4380 4400 4420 4440 4460 4480\n",
      " 4500 4520 4540 4560 4580 4600 4620 4640 4660 4680 4700 4720 4740 4760\n",
      " 4780 4800 4820 4840 4860 4880 4900 4920 4940 4960 4980 5000 5020 5040\n",
      " 5060 5080 5100 5120 5140 5160 5180 5200 5220 5240 5260 5280 5300 5320\n",
      " 5340 5360 5380 5400 5420 5440 5460 5480 5500 5520 5540 5560 5580 5600\n",
      " 5620 5640 5660 5680 5700 5720 5740 5760 5780 5800 5820 5840 5860 5880\n",
      " 5900 5920 5940 5960 5980 6000 6020 6040 6060 6080 6100 6120 6140 6160\n",
      " 6180 6200 6220 6240 6260 6280 6300 6320 6340 6360 6380 6400 6420 6440\n",
      " 6460 6480 6500 6520 6540 6560 6580 6600 6620 6640 6660 6680 6700 6720\n",
      " 6740 6760 6780 6800 6820 6840 6860 6880 6900 6920 6940 6960 6980 7000\n",
      " 7020 7040 7060 7080 7100 7120 7140 7160 7180 7200 7220 7240 7260 7280\n",
      " 7300 7320 7340 7360 7380 7400 7420 7440 7460 7480 7500 7520 7540 7560\n",
      " 7580 7600 7620 7640 7660 7680 7700 7720 7740 7760 7780 7800 7820 7840\n",
      " 7860 7880 7900 7920 7940 7960 7980 8000 8020 8040 8060 8080 8100 8120\n",
      " 8140 8160 8180 8200 8220 8240 8260 8280 8300 8320 8340 8360 8380 8400\n",
      " 8420 8440 8460 8480 8500 8520 8540 8560 8580 8600 8620 8640 8660 8680\n",
      " 8700 8720 8740 8760 8780 8800 8820 8840 8860 8880 8900 8920 8940 8960\n",
      " 8980]\n",
      "shuffled indices [0:10] [5957 1550  251 4606 4787 4027 3640 6779 8454 1018]\n",
      "batches_indices 450\n",
      "batches_indices[0].shape (20,)\n",
      "loss 2.60689982853195\n",
      "it.shape= (3073, 10)\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "\n",
    "num_train=train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "batch_size=20\n",
    "reg=1e1\n",
    "learning_rate=1e-3\n",
    "print('num_train', num_train)\n",
    "print('num_features', num_features)\n",
    "print('num_classe',num_classes)\n",
    "print('batch_size',batch_size)\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "sections = np.arange(batch_size, num_train, batch_size)\n",
    "print('sections',sections)\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "print('shuffled indices [0:10]', shuffled_indices[0:10])\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n",
    "print('batches_indices', len(batches_indices))\n",
    "print('batches_indices[0].shape', batches_indices[0].shape)\n",
    "\n",
    "target_index=train_y[batches_indices[0]]\n",
    "X=train_X[batches_indices[0]]\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, reg, X, target_index)\n",
    "print('loss',loss)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, reg,X,target_index), W)\n",
    "#loss_history = classifier.fit(train_X, train_y, epochs=1, learning_rate=1e-3, batch_size=1, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1 3 5]\n",
      "[ 2  6 10]\n",
      "[5957 1550  251 4606 4787 4027 3640 6779 8454 1018 4551 5584 1874 8977\n",
      " 4632 6463 1626 5886 2762 5644]\n",
      "[7 3 7 3 0 6 0 6 0 8 2 6 3 1 6 4 6 6 1 7]\n"
     ]
    }
   ],
   "source": [
    "t=np.arange(10)*2\n",
    "i=np.array([1,3,5])\n",
    "print(t)\n",
    "print(i)\n",
    "print(t[i])\n",
    "print(batches_indices[0])\n",
    "print(train_y[batches_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first W = none\n",
      "nake W\n",
      "Accuracy:  0.109\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first W = none\n",
      "w.shape (3073, 10)\n",
      "w 0 initial  0.0005856266114753865\n",
      "loss history []\n",
      "Accuracy:  0.09\n",
      "loss history [2.3975604821273415]\n",
      "Accuracy:  0.09\n",
      "loss history [2.3973204336124048, 2.3296166396322975]\n",
      "Accuracy:  0.111\n",
      "loss history [2.39696586959767, 2.329661773309276, 2.3102211703743585]\n",
      "Accuracy:  0.113\n",
      "loss history [2.3971663560650707, 2.3308748678376943, 2.31030217868913, 2.304044163451739]\n",
      "Accuracy:  0.111\n",
      "loss history [2.397390411349983, 2.3307726824002764, 2.3102335936077916, 2.305381604646697, 2.3024844480376867]\n",
      "Accuracy:  0.129\n",
      "loss history [2.397216464864593, 2.3305868189703434, 2.3100494142691055, 2.3041326278270855, 2.302633924480031, 2.3023718987950996]\n",
      "Accuracy:  0.132\n",
      "loss history [2.3969481952465497, 2.3308203772790446, 2.310804413565137, 2.3046706880934935, 2.3026148863089992, 2.302486666632692, 2.302079983498984]\n",
      "Accuracy:  0.114\n",
      "loss history [2.3970254618182483, 2.3301971226773217, 2.3105071607006265, 2.3051875493219836, 2.3026837159770435, 2.3017974761162416, 2.3027615294938917, 2.302276297530774]\n",
      "Accuracy:  0.134\n",
      "loss history [2.3979596766821585, 2.330440424605272, 2.3114094366005022, 2.3040715198082897, 2.302679664170431, 2.302217512996382, 2.301269622849728, 2.3021917596221266, 2.3022235186317546]\n",
      "Accuracy:  0.119\n",
      "w 0 final  0.0005856266114753865\n"
     ]
    }
   ],
   "source": [
    "#TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "W= 0.001 * np.random.randn(num_features, num_classes)\n",
    "print('w.shape',W.shape)\n",
    "print('w 0 initial ', W[0,0])\n",
    "\n",
    "for i in range(10):\n",
    "    classifier.W=W\n",
    "    #print(classifier.W[0,0])\n",
    "    loss_history = classifier.fit(train_X, train_y, epochs=i, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    print('loss history',loss_history)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "print('w 0 final ', W[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f886b221b00>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_X.shape (1000, 3073)\n",
      "val_X[10] [2 9 8 4 6 5 3 3 3 1]\n",
      "pred[10] [2 4 4 3 5 7 5 1 2 5]\n",
      "Accuracy:  0.119\n"
     ]
    }
   ],
   "source": [
    "print('val_X.shape',val_X.shape)\n",
    "pred = classifier.predict(val_X)\n",
    "print('val_X[10]',val_y[:10])\n",
    "print('pred[10]', pred[:10])\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.119\n",
      "Accuracy after training:  0.116\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=20, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-16dee5d2c6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbest_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_X.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_y.shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_X.shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "#learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "learning_rates = [2e-1, 1e-1, 1e-2, 1e-3]\n",
    "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 5e-8, 1e-8]\n",
    "\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "print('train_X.shape', train_X.shape)\n",
    "print('train_y.shape',train_y.shape)\n",
    "print('val_X.shape',val_X.shape)\n",
    "print('val_y.shape',val_y.shape)\n",
    "results = np.zeros((len(learning_rates),len(reg_strengths)),dtype=np.float)\n",
    "                \n",
    "#W= 0.001 * np.random.randn(num_features, num_classes)\n",
    "classifier1 = linear_classifer.LinearSoftmaxClassifier()\n",
    "print('w 0 initial ', W[0,0])\n",
    "\n",
    "for i_lr,lr in enumerate(learning_rates):\n",
    "    for i_rs, rs in enumerate (reg_strengths):\n",
    "        classifier1.W=W\n",
    "        loss_history=classifier1.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier1.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        results[i_lr][i_rs]=accuracy\n",
    "        print('learning rate =', lr,  'reg str', rs, ' Accuracy',   accuracy)\n",
    "print('results', results)\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.209 0.218 0.225 0.25  0.218 0.194 0.21 ]\n",
      " [0.244 0.239 0.242 0.241 0.237 0.248 0.246]\n",
      " [0.245 0.245 0.245 0.243 0.242 0.244 0.242]\n",
      " [0.227 0.226 0.227 0.227 0.227 0.226 0.227]]\n",
      "[3 5 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "max_index=np.argmax(results,axis=1)\n",
    "print(max_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2b2415c4c0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreg_strengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#best_hyper=np.array([[1e-1, 1e-2, 1e-3, 1e-4, 1e-5],[1e-3, 1e-4, 1e-5, 1e-6, 1e-7]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w 0 initial '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_strengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rates = [2e-1, 1e-1, 1e-2, 1e-3]\n",
    "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 5e-8, 1e-8]\n",
    "#best_hyper=np.array([[1e-1, 1e-2, 1e-3, 1e-4, 1e-5],[1e-3, 1e-4, 1e-5, 1e-6, 1e-7]])\n",
    "print('w 0 initial ', W[0,0])\n",
    "classifier1.W=W\n",
    "loss_history=classifier1.fit(train_X, train_y, epochs=num_epochs, learning_rate=learning_rates[1], batch_size=batch_size, reg=reg_strengths[5])\n",
    "pred = classifier1.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(' Accuracy',   accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "#test_pred = best_classifier.predict(test_X)\n",
    "test_pred = classifier1.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
