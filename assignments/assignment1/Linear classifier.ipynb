{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_X.shape initial', (10000, 32, 32, 3))\n",
      "('train_x.mean', 0.446616470588233)\n",
      "('indices initial', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
      "('indices after shuffle', array([6252, 4684, 1731, 4742, 4521, 6340,  576, 5202, 6363,  439]))\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    print('train_x.mean',mean_image[0])\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    #print('train_flat_ones.shape', train_flat_with_ones.shape)\n",
    "    #print('train_flat_ones[0]', train_flat_with_ones[0])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n",
    "print('train_X.shape initial', train_X.shape) #expect 1000,32,32,3\n",
    "#print('train_X[0,0]', train_X[0,0])\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_sum(x):\n",
    "    #print('x.shape',x.shape)\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "probs = linear_classifer.softmax(np.array([1, 1, 1]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0][0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "1\n",
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.zeros(5)\n",
    "print(a)\n",
    "print(np.ndim(a))\n",
    "b=a.reshape(1,-1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('probs', array([[0.57611688, 0.21194156, 0.21194156]]))\n",
      "('loss=', array([1.55144471]))\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "print('probs',probs)\n",
    "#linear_classifer.cross_entropy_loss(probs, 1)\n",
    "loss=linear_classifer.cross_entropy_loss(probs, np.array([[1]]))\n",
    "print('loss=',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss=', 1.551444713932051)\n",
      "('grad', array([ 0.57611688, -0.78805844,  0.21194156]))\n",
      "('it.shape=', (3,))\n",
      "Gradient check passed!\n",
      "----------star check batch------\n",
      "('loss=', 1.051444713932051)\n",
      "('grad', array([[ 0.28805844, -0.39402922,  0.10597078],\n",
      "       [-0.21194156,  0.10597078,  0.10597078]]))\n",
      "('it.shape=', (2, 3))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n",
    "\n",
    "print('----------star check batch------')\n",
    "predictions=np.array([[1,0,0],[1,0,0]],dtype=np.float)\n",
    "target_index=np.array([[1],[0]],dtype=np.int)\n",
    "loss, grad =linear_classifer.softmax_with_cross_entropy_batch(predictions, target_index)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, np.array([1,0]), np.array([[1,0,0],[1,0,0]],np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- batch 2, num 4 ----------\n",
      "('predictions', array([[ 1.,  2., -1.,  1.],\n",
      "       [ 1.,  2., -1., -1.]]))\n",
      "('it.shape=', (2, 4))\n",
      "Gradient check passed!\n",
      "----------- batch 3, num 4 ----------\n",
      "('it.shape=', (3, 4))\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "\n",
    "print('----------- batch 2, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 2\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "print('----------- batch 3, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss', 0.5472365148987376)\n",
      "('dW=', array([[-0.22019927,  0.22019927],\n",
      "       [-0.20332316,  0.20332316],\n",
      "       [ 0.23874859, -0.23874859]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 4\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss',loss)\n",
    "print('dW=',dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('target_index', array([1, 1, 1, 1, 1]))\n",
      "('loss SoftMax', 0.44141919750255204)\n",
      "('dW Softmax=', array([[-0.16896493,  0.16896493],\n",
      "       [-0.15906129,  0.15906129],\n",
      "       [ 0.19099887, -0.19099887]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('loss L2', 0.12)\n",
      "('dW  L2=', array([[ 0.02,  0.04],\n",
      "       [-0.02,  0.02],\n",
      "       [ 0.02,  0.04]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# this is my test\n",
    "batch_size = 5\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print('target_index',target_index)\n",
    "\n",
    "loss_sm, dW_sm = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss SoftMax',loss_sm)\n",
    "print('dW Softmax=',dW_sm)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "loss_l2, dW_l2=linear_classifer.l2_regularization(W, 0.01)\n",
    "print('loss L2',loss_l2)\n",
    "print('dW  L2=',dW_l2)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n",
    "\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, 0.01, X, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, 0.01,X,target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num_train', 900)\n",
      "('num_features', 3073)\n",
      "('num_classe', 10)\n",
      "('batch_size', 20)\n",
      "('sections', array([ 20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240, 260,\n",
      "       280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520,\n",
      "       540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780,\n",
      "       800, 820, 840, 860, 880]))\n",
      "('shuffled indices [0:10]', array([722, 826, 642, 368,  46, 492, 303, 314, 388,  10]))\n",
      "('batches_indices', 45)\n",
      "('batches_indices[0].shape', (20,))\n",
      "('loss', 2.605083910175978)\n",
      "('it.shape=', (3073, 10))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "\n",
    "num_train=train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "batch_size=20\n",
    "reg=1e1\n",
    "learning_rate=1e-3\n",
    "print('num_train', num_train)\n",
    "print('num_features', num_features)\n",
    "print('num_classe',num_classes)\n",
    "print('batch_size',batch_size)\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "sections = np.arange(batch_size, num_train, batch_size)\n",
    "print('sections',sections)\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "print('shuffled indices [0:10]', shuffled_indices[0:10])\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n",
    "print('batches_indices', len(batches_indices))\n",
    "print('batches_indices[0].shape', batches_indices[0].shape)\n",
    "\n",
    "target_index=train_y[batches_indices[0]]\n",
    "X=train_X[batches_indices[0]]\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, reg, X, target_index)\n",
    "print('loss',loss)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, reg,X,target_index), W)\n",
    "#loss_history = classifier.fit(train_X, train_y, epochs=1, learning_rate=1e-3, batch_size=1, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1 3 5]\n",
      "[ 2  6 10]\n",
      "[853 402  76  22 568 763 452 838 542 331  55 187 616  47 181 861 670  66\n",
      " 803 716]\n",
      "[5 9 1 4 0 3 0 0 2 7 2 6 1 5 5 3 2 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "t=np.arange(10)*2\n",
    "i=np.array([1,3,5])\n",
    "print(t)\n",
    "print(i)\n",
    "print(t[i])\n",
    "print(batches_indices[0])\n",
    "print(train_y[batches_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397225\n",
      "Epoch 1, loss: 2.329579\n",
      "Epoch 2, loss: 2.310828\n",
      "Epoch 3, loss: 2.304595\n",
      "Epoch 4, loss: 2.302691\n",
      "Epoch 5, loss: 2.303239\n",
      "Epoch 6, loss: 2.303377\n",
      "Epoch 7, loss: 2.302525\n",
      "Epoch 8, loss: 2.302522\n",
      "Epoch 9, loss: 2.301590\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc0b3d62a90>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val_{.shape', (1000, 3073))\n"
     ]
    }
   ],
   "source": [
    "print('val_{.shape',val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-72d5fc69146b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's check how it performs on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pasha_dlcouse_ai/dlcourse_ai/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# TODO Implement class prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not implemented!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Not implemented!"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
