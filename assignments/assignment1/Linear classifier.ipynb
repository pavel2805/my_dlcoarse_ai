{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Задание 1.2 - Линейный классификатор (Linear classifier)\n\nВ этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\nТот класс, у которого эта сумма больше, и является предсказанием модели.\n\nВ этом задании вы:\n- потренируетесь считать градиенты различных многомерных функций\n- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n- реализуете процесс тренировки линейного классификатора\n- подберете параметры тренировки на практике\n\nНа всякий случай, еще раз ссылка на туториал по numpy:  \nhttp://cs231n.github.io/python-numpy-tutorial/"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from dataset import load_svhn, random_split_train_val\nfrom gradient_check import check_gradient\nfrom metrics import multiclass_accuracy \nimport linear_classifer",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Как всегда, первым делом загружаем данные\n\nМы будем использовать все тот же SVHN."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def prepare_for_linear_classifier(train_X, test_X):\n    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n    \n    # Subtract mean\n    mean_image = np.mean(train_flat, axis = 0)\n    print('train_x.mean',mean_image[0])\n    train_flat -= mean_image\n    test_flat -= mean_image\n    \n    # Add another channel with ones as a bias term\n    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n    #print('train_flat_ones.shape', train_flat_with_ones.shape)\n    #print('train_flat_ones[0]', train_flat_with_ones[0])\n    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n    return train_flat_with_ones, test_flat_with_ones\n    \n\ntrain_X_t, train_y_t, test_X, test_y = load_svhn(\"data\", max_train=11000, max_test=1000)\n\ntrain_X = train_X_t[0:10000]\ntrain_y =train_y_t[0:10000]\ntest_X = train_X_t[10000:11000]\ntest_y = train_y_t[10000:11000]    \n\n#train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n\nprint('train_X.shape initial', train_X.shape) #expect 1000,32,32,3\n#print('train_X[0,0]', train_X[0,0])\ntrain_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n# Split train into train and val\ntrain_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('train_X.shape initial', (10000, 32, 32, 3))\n('train_x.mean', 0.45317529411763996)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Играемся с градиентами!\n\nВ этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n\nВсе функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \nОни будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n```\ndef f(x):\n    \"\"\"\n    Computes function and analytic gradient at x\n    \n    x: np array of float, input to the function\n    \n    Returns:\n    value: float, value of the function \n    grad: np array of float, same shape as x\n    \"\"\"\n    ...\n    \n    return value, grad\n```\n\nНеобходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n\nМы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n\nНапишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n\nВсе функции приведенные в следующей клетке должны проходить gradient check."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def square(x):\n    return float(x*x), 2*x\n\ncheck_gradient(square, np.array([3.0]))\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('it.shape=', (1,))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def array_sum(x):\n    #print('x.shape',x.shape)\n    assert x.shape == (2,), x.shape\n    return np.sum(x), np.ones_like(x)\n\ncheck_gradient(array_sum, np.array([3.0, 2.0]))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('it.shape=', (2,))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO: Implement check_gradient function in gradient_check.py\n# All the functions below should pass the gradient check\n\ndef square(x):\n    return float(x*x), 2*x\n\ncheck_gradient(square, np.array([3.0]))\n\ndef array_sum(x):\n    assert x.shape == (2,), x.shape\n    return np.sum(x), np.ones_like(x)\n\ncheck_gradient(array_sum, np.array([3.0, 2.0]))\n\ndef array_2d_sum(x):\n    assert x.shape == (2,2)\n    return np.sum(x), np.ones_like(x)\n\ncheck_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('it.shape=', (1,))\nGradient check passed!\n('it.shape=', (2,))\nGradient check passed!\n('it.shape=', (2, 2))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Начинаем писать свои функции, считающие аналитический градиент\n\nТеперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n\n**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n\nК счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n```\npredictions -= np.max(predictions)\n```\n(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO Implement softmax and cross-entropy for single sample\nprobs = linear_classifer.softmax(np.array([-10, 0, 10]))\nprobs = linear_classifer.softmax(np.array([1, 1, 1]))\n# Make sure it works for big numbers too!\nprobs = linear_classifer.softmax(np.array([1000, 0, 0]))\nassert np.isclose(probs[0][0], 1.0)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a=np.zeros(5)\nprint(a)\nprint(np.ndim(a))\nb=a.reshape(1,-1)\nprint(b)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0. 0. 0. 0. 0.]\n1\n[[0. 0. 0. 0. 0.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\nВ общем виде cross-entropy определена следующим образом:\n![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n\nгде x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \nВ нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n\nЭто позволяет реализовать функцию проще!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "probs = linear_classifer.softmax(np.array([1, 0, 0]))\nprint('probs',probs)\n#linear_classifer.cross_entropy_loss(probs, 1)\nloss=linear_classifer.cross_entropy_loss(probs, np.array([[1]]))\nprint('loss=',loss)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('probs', array([[0.57611688, 0.21194156, 0.21194156]]))\n('loss=', array([1.55144471]))\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "После того как мы реализовали сами функции, мы можем реализовать градиент.\n\nОказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n\nЭта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from dataset import load_svhn, random_split_train_val\nfrom gradient_check import check_gradient, check_gradient_batch\nfrom metrics import multiclass_accuracy \nimport linear_classifer\n\n# TODO Implement combined function or softmax and cross entropy and produces gradient\nloss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\nprint('loss=',loss)\nprint('grad',grad)\ncheck_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n\nprint('----------star check batch------')\npredictions=np.array([[1,0,0],[1,0,0]],dtype=np.float)\ntarget_index=np.array([[1],[0]],dtype=np.int)\nloss, grad =linear_classifer.softmax_with_cross_entropy_batch(predictions, target_index)\nprint('loss=',loss)\nprint('grad',grad)\ncheck_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, np.array([1,0]), np.array([[1,0,0],[1,0,0]],np.float))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('loss=', 1.551444713932051)\n('grad', array([ 0.57611688, -0.78805844,  0.21194156]))\n('it.shape=', (3,))\nGradient check passed!\n----------star check batch------\n('loss=', 1.051444713932051)\n('grad', array([[ 0.28805844, -0.39402922,  0.10597078],\n       [-0.21194156,  0.10597078,  0.10597078]]))\n('it.shape=', (2, 3))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n\nПоэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n\nСледующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n\nФинальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO Extend combined function so it can receive a 2d array with batch of samples\nnp.random.seed(42)\n# Test batch_size = 1\n\nprint('----------- batch 2, num 4 ----------')\nnum_classes = 4\nbatch_size = 2\npredictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\nprint('predictions',predictions)\ntarget_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\ncheck_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n\n# Test batch_size = 3\nprint('----------- batch 3, num 4 ----------')\nnum_classes = 4\nbatch_size = 3\npredictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\ntarget_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\ncheck_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n\n# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\nprobs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\nassert np.all(np.isclose(probs[:, 0], 1.0))\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "----------- batch 2, num 4 ----------\n('predictions', array([[ 1.,  2., -1.,  1.],\n       [ 1.,  2., -1., -1.]]))\n('it.shape=', (2, 4))\nGradient check passed!\n----------- batch 3, num 4 ----------\n('it.shape=', (3, 4))\nGradient check passed!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Наконец, реализуем сам линейный классификатор!\n\nsoftmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n\nОн делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n\nТаким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n\n`predictions = X * W`, где `*` - матричное умножение.\n\nРеализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\nbatch_size = 4\nnum_features = 3\n\nnum_classes = 2\nnp.random.seed(42)\nW = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\nX = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\ntarget_index = np.ones(batch_size, dtype=np.int)\n\nloss, dW = linear_classifer.linear_softmax(X, W, target_index)\nprint('loss',loss)\nprint('dW=',dW)\ncheck_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('loss', 0.5472365148987376)\n('dW=', array([[-0.22019927,  0.22019927],\n       [-0.20332316,  0.20332316],\n       [ 0.23874859, -0.23874859]]))\n('it.shape=', (3, 2))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### И теперь регуляризация\n\nМы будем использовать L2 regularization для весов как часть общей функции ошибки.\n\nНапомним, L2 regularization определяется как\n\nl2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n\nРеализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO Implement l2_regularization function that implements loss for L2 regularization\nlinear_classifer.l2_regularization(W, 0.01)\ncheck_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('it.shape=', (3, 2))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from dataset import load_svhn, random_split_train_val\nfrom gradient_check import check_gradient, check_gradient_batch\nfrom metrics import multiclass_accuracy \nimport linear_classifer\n\n# this is my test\nbatch_size = 5\nnum_features = 3\n\nnum_classes = 2\nnp.random.seed(42)\nW = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\nX = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\ntarget_index = np.ones(batch_size, dtype=np.int)\nprint('target_index',target_index)\n\nloss_sm, dW_sm = linear_classifer.linear_softmax(X, W, target_index)\nprint('loss SoftMax',loss_sm)\nprint('dW Softmax=',dW_sm)\ncheck_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\nloss_l2, dW_l2=linear_classifer.l2_regularization(W, 0.01)\nprint('loss L2',loss_l2)\nprint('dW  L2=',dW_l2)\ncheck_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n\nloss,dW=linear_classifer.linear_softmax_l2(W, 0.01, X, target_index)\ncheck_gradient(lambda w: linear_classifer.linear_softmax_l2(w, 0.01,X,target_index), W)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('target_index', array([1, 1, 1, 1, 1]))\n('loss SoftMax', 0.44141919750255204)\n('dW Softmax=', array([[-0.16896493,  0.16896493],\n       [-0.15906129,  0.15906129],\n       [ 0.19099887, -0.19099887]]))\n('it.shape=', (3, 2))\nGradient check passed!\n('loss L2', 0.12)\n('dW  L2=', array([[ 0.02,  0.04],\n       [-0.02,  0.02],\n       [ 0.02,  0.04]]))\n('it.shape=', (3, 2))\nGradient check passed!\n('it.shape=', (3, 2))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "classifier = linear_classifer.LinearSoftmaxClassifier()\n\nnum_train=train_X.shape[0]\nnum_features = train_X.shape[1]\nnum_classes = np.max(train_y)+1\nbatch_size=20\nreg=1e1\nlearning_rate=1e-3\nprint('num_train', num_train)\nprint('num_features', num_features)\nprint('num_classe',num_classes)\nprint('batch_size',batch_size)\nW = 0.001 * np.random.randn(num_features, num_classes)\nsections = np.arange(batch_size, num_train, batch_size)\nprint('sections',sections)\nshuffled_indices = np.arange(num_train)\nnp.random.shuffle(shuffled_indices)\nprint('shuffled indices [0:10]', shuffled_indices[0:10])\nbatches_indices = np.array_split(shuffled_indices, sections)\nprint('batches_indices', len(batches_indices))\nprint('batches_indices[0].shape', batches_indices[0].shape)\n\ntarget_index=train_y[batches_indices[0]]\nX=train_X[batches_indices[0]]\nloss,dW=linear_classifer.linear_softmax_l2(W, reg, X, target_index)\nprint('loss',loss)\ncheck_gradient(lambda w: linear_classifer.linear_softmax_l2(w, reg,X,target_index), W)\n#loss_history = classifier.fit(train_X, train_y, epochs=1, learning_rate=1e-3, batch_size=1, reg=1e1)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('num_train', 9000)\n('num_features', 3073)\n('num_classe', 10)\n('batch_size', 20)\n('sections', array([  20,   40,   60,   80,  100,  120,  140,  160,  180,  200,  220,\n        240,  260,  280,  300,  320,  340,  360,  380,  400,  420,  440,\n        460,  480,  500,  520,  540,  560,  580,  600,  620,  640,  660,\n        680,  700,  720,  740,  760,  780,  800,  820,  840,  860,  880,\n        900,  920,  940,  960,  980, 1000, 1020, 1040, 1060, 1080, 1100,\n       1120, 1140, 1160, 1180, 1200, 1220, 1240, 1260, 1280, 1300, 1320,\n       1340, 1360, 1380, 1400, 1420, 1440, 1460, 1480, 1500, 1520, 1540,\n       1560, 1580, 1600, 1620, 1640, 1660, 1680, 1700, 1720, 1740, 1760,\n       1780, 1800, 1820, 1840, 1860, 1880, 1900, 1920, 1940, 1960, 1980,\n       2000, 2020, 2040, 2060, 2080, 2100, 2120, 2140, 2160, 2180, 2200,\n       2220, 2240, 2260, 2280, 2300, 2320, 2340, 2360, 2380, 2400, 2420,\n       2440, 2460, 2480, 2500, 2520, 2540, 2560, 2580, 2600, 2620, 2640,\n       2660, 2680, 2700, 2720, 2740, 2760, 2780, 2800, 2820, 2840, 2860,\n       2880, 2900, 2920, 2940, 2960, 2980, 3000, 3020, 3040, 3060, 3080,\n       3100, 3120, 3140, 3160, 3180, 3200, 3220, 3240, 3260, 3280, 3300,\n       3320, 3340, 3360, 3380, 3400, 3420, 3440, 3460, 3480, 3500, 3520,\n       3540, 3560, 3580, 3600, 3620, 3640, 3660, 3680, 3700, 3720, 3740,\n       3760, 3780, 3800, 3820, 3840, 3860, 3880, 3900, 3920, 3940, 3960,\n       3980, 4000, 4020, 4040, 4060, 4080, 4100, 4120, 4140, 4160, 4180,\n       4200, 4220, 4240, 4260, 4280, 4300, 4320, 4340, 4360, 4380, 4400,\n       4420, 4440, 4460, 4480, 4500, 4520, 4540, 4560, 4580, 4600, 4620,\n       4640, 4660, 4680, 4700, 4720, 4740, 4760, 4780, 4800, 4820, 4840,\n       4860, 4880, 4900, 4920, 4940, 4960, 4980, 5000, 5020, 5040, 5060,\n       5080, 5100, 5120, 5140, 5160, 5180, 5200, 5220, 5240, 5260, 5280,\n       5300, 5320, 5340, 5360, 5380, 5400, 5420, 5440, 5460, 5480, 5500,\n       5520, 5540, 5560, 5580, 5600, 5620, 5640, 5660, 5680, 5700, 5720,\n       5740, 5760, 5780, 5800, 5820, 5840, 5860, 5880, 5900, 5920, 5940,\n       5960, 5980, 6000, 6020, 6040, 6060, 6080, 6100, 6120, 6140, 6160,\n       6180, 6200, 6220, 6240, 6260, 6280, 6300, 6320, 6340, 6360, 6380,\n       6400, 6420, 6440, 6460, 6480, 6500, 6520, 6540, 6560, 6580, 6600,\n       6620, 6640, 6660, 6680, 6700, 6720, 6740, 6760, 6780, 6800, 6820,\n       6840, 6860, 6880, 6900, 6920, 6940, 6960, 6980, 7000, 7020, 7040,\n       7060, 7080, 7100, 7120, 7140, 7160, 7180, 7200, 7220, 7240, 7260,\n       7280, 7300, 7320, 7340, 7360, 7380, 7400, 7420, 7440, 7460, 7480,\n       7500, 7520, 7540, 7560, 7580, 7600, 7620, 7640, 7660, 7680, 7700,\n       7720, 7740, 7760, 7780, 7800, 7820, 7840, 7860, 7880, 7900, 7920,\n       7940, 7960, 7980, 8000, 8020, 8040, 8060, 8080, 8100, 8120, 8140,\n       8160, 8180, 8200, 8220, 8240, 8260, 8280, 8300, 8320, 8340, 8360,\n       8380, 8400, 8420, 8440, 8460, 8480, 8500, 8520, 8540, 8560, 8580,\n       8600, 8620, 8640, 8660, 8680, 8700, 8720, 8740, 8760, 8780, 8800,\n       8820, 8840, 8860, 8880, 8900, 8920, 8940, 8960, 8980]))\n('shuffled indices [0:10]', array([5957, 1550,  251, 4606, 4787, 4027, 3640, 6779, 8454, 1018]))\n('batches_indices', 450)\n('batches_indices[0].shape', (20,))\n('loss', 2.6095638566644457)\n('it.shape=', (3073, 10))\nGradient check passed!\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "t=np.arange(10)*2\ni=np.array([1,3,5])\nprint(t)\nprint(i)\nprint(t[i])\nprint(batches_indices[0])\nprint(train_y[batches_indices[0]])",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 0  2  4  6  8 10 12 14 16 18]\n[1 3 5]\n[ 2  6 10]\n[5957 1550  251 4606 4787 4027 3640 6779 8454 1018 4551 5584 1874 8977\n 4632 6463 1626 5886 2762 5644]\n[1 1 4 4 1 1 1 2 1 6 3 9 4 2 5 1 8 6 1 3]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Тренировка!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Градиенты в порядке, реализуем процесс тренировки!"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TODO: Implement LinearSoftmaxClassifier.fit function\nclassifier = linear_classifer.LinearSoftmaxClassifier()\nloss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "first W = none\nnake W\nEpoch 0, loss: 2.397706\nEpoch 1, loss: 2.330696\nEpoch 2, loss: 2.310452\nEpoch 3, loss: 2.304400\nEpoch 4, loss: 2.303051\nEpoch 5, loss: 2.303128\nEpoch 6, loss: 2.302073\nEpoch 7, loss: 2.302643\nEpoch 8, loss: 2.302062\nEpoch 9, loss: 2.301643\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's look at the loss history!\nplt.plot(loss_history)",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "[<matplotlib.lines.Line2D at 0x7f074928e710>]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHhxJREFUeJzt3Xt8XOV95/HPb0Y3S7JkSZYxli8amQTbITYWwnIgl11IE5K05daE5gK57IakTVNISduE5NXshqav0M3SpEm21Am5kLBpsthk2UBCvJQspQkGSTYYW1x9N8YWki+yLUszmt/+MUfWBdkaySMfac73/Xrx0uicZ4bfzAu+z9HzPPMcc3dERCQaYmEXICIiZ49CX0QkQhT6IiIRotAXEYkQhb6ISIQo9EVEIkShLyISIQp9EZEIUeiLiERIQdgFjDR79myvr68PuwwRkWmltbX1VXevHavdlAv9+vp6Wlpawi5DRGRaMbOd2bQbc3jHzBaY2SNm1m5mW8zsptO0vdjM+s3sj4Yc+7CZvRD88+HsyhcRkcmQzZV+CrjF3dvMbCbQambr3X3r0EZmFgduBx4acqwa+BLQBHjw3Pvd/WDO3oGIiGRtzCt9d9/n7m3B426gHagbpemngbXAgSHH3gmsd/euIOjXA1eccdUiIjIh41q9Y2b1wEpgw4jjdcDVwJ0jnlIH7B7y+x5G7zBEROQsyDr0zayczJX8ze5+ZMTprwN/7e79I582yku9ZgN/M7vRzFrMrKWjoyPbkkREZJyyWr1jZoVkAv8ed183SpMm4F/MDGA28G4zS5G5sv8PQ9rNB34z8snuvgZYA9DU1KS7uoiITJIxQ98ySX4X0O7ud4zWxt0TQ9r/APiFu/88mMj9OzOrCk6/A/j8GVctIiITks2V/qXA9cBmM9sUHLsVWAjg7iPH8U9y9y4zuw14Mjj0ZXfvOoN6T+nQ8T5++NudXL50DhfUVU7Gv0JEZNobM/Td/TFGH5s/VfuPjPj9e8D3xl3ZOMVixjcefp5+d4W+iMgp5M3eOxUlhSybV8GGbZ1hlyIiMmXlTegDNCdq2Lj7ECeSIxcRiYgI5F3oV9OXSvP0nsNhlyIiMiXlVeivSlRjhoZ4REROIa9Cf1ZpEeefM5MN2ydlgZCIyLSXV6EPmSGe1p0HSfanwy5FRGTKyb/Qb6ihJ9nP5r0a1xcRGSnvQn9VohqADds0xCMiMlLehf7s8mLOm1POhu2azBURGSnvQh8yV/stOw6S0ri+iMgweRn6zYlqjvam2Lpv5A7QIiLRlpehv7qhBoAntHRTRGSYvAz9cypKqK8p5XFN5oqIDJOXoQ+ZfXie3NFFOq17soiIDMjb0F+VqOZwT5JnX+kOuxQRkSkjb0O/uSFYr6+lmyIiJ+Vt6M+vKqVu1gxN5oqIDJG3oQ+Zq/0ntnfhrnF9ERHI89Bfnaih81gfLx44GnYpIiJTQl6H/sA+PI9riEdEBMjz0F9UU8o5FcUa1xcRCeR16JsZzYkaNmzr1Li+iAh5HvqQmcw90N3Ljs7jYZciIhK6/A/9k/vra72+iEjeh/7i2nJmlxfpvrkiIkQg9M2MVYlqTeaKiBCB0IfM5mt7D/Wwu0vj+iISbdEI/ZP78OhqX0SiLRKh//o5M5lVWqjJXBGJvEiEfixmXFxfrSt9EYm8SIQ+ZJZu7uo6zr7DPWGXIiISmsiEvu6bKyKSReib2QIze8TM2s1si5ndNEqbK83saTPbZGYtZvbmIef+Pnheu5n9o5lZrt9ENpaeW8HM4gLdN1dEIq0gizYp4BZ3bzOzmUCrma13961D2jwM3O/ubmbLgZ8BS8zsEuBSYHnQ7jHgbcBvcvYOshSPGU31VbqTlohE2phX+u6+z93bgsfdQDtQN6LNUR/c0awMGHjsQAlQBBQDhcD+3JQ+fs0NNWzrOMaB7hNhlSAiEqpxjembWT2wEtgwyrmrzexZ4AHgYwDu/jvgEWBf8M9D7t5+ZiVP3MA+PE9uPxhWCSIioco69M2sHFgL3OzuR0aed/f73H0JcBVwW/Cc84ClwHwyfx1cZmZvHeW1bwzmAlo6Ojom9k6ycEFdJaVFcQ3xiEhkZRX6ZlZIJvDvcfd1p2vr7o8Ci81sNnA18Hgw/HMU+CWwepTnrHH3Jndvqq2tHfebyFZhPMZFi6rYoMlcEYmobFbvGHAX0O7ud5yizXkDq3LMrJHMGH4nsAt4m5kVBB3H28jMCYSmOVHNc/u76TrWF2YZIiKhyGb1zqXA9cBmM9sUHLsVWAjg7ncC1wI3mFkS6AGuC1by3AtcBmwmM6n7K3f/Pzl+D+PSHKzXf3JHF+98w9wwSxEROevGDH13fww47dp6d78duH2U4/3AJyZc3SRYPr+S4oIYG7Yp9EUkeiLzjdwBxQVxGhdqvb6IRFPkQh9gVaKarfuOcLgnGXYpIiJnVSRDv7mhGndo2aFVPCISLZEM/caFVRTFY9p8TUQiJ5KhX1IYZ8WCSh5X6ItIxEQy9CFz39xn9h7maG8q7FJERM6ayIb+qkQ1/Wmndaf24RGR6Ihs6F+0qIp4zHTfXBGJlMiGfllxAW+sq9RkrohESmRDHzJLN5/ac4ievv6wSxEROSsiHfqrEzUk+52NuzSuLyLREOnQv6i+ipihpZsiEhmRDv2KkkKWzavgCe3DIyIREenQh8x6/Y27DtGb0ri+iOQ/hX6imt5Umqd2Hw67FBGRSRf50L+4PnOzdK3XF5EoiHzoV5UVsWTuTDZoMldEIiDyoQ+ZIZ7WnQdJ9qfDLkVEZFIp9MncN7cn2c/mvRrXF5H8ptAns/kawIZtGuIRkfym0AdmlxezuLZM980Vkbyn0A80N9TQsuMgKY3ri0geU+gHmhPVHO1N0b6vO+xSREQmjUI/sLqhBkBDPCKS1xT6gXMqSqivKeVxTeaKSB5T6A+xKlHNkzu6SKc97FJERCaFQn+I5kQNh3uSPLdf4/oikp8U+kM0N2gfHhHJbwr9IeZXlVI3a4b24RGRvKXQH6E5Uc0T27tw17i+iOQfhf4IzQ3VdB7r48UDR8MuRUQk5xT6IzQnBtbra4hHRPLPmKFvZgvM7BEzazezLWZ20yhtrjSzp81sk5m1mNmbh5xbaGa/Dp6/1czqc/sWcmtRTSnnVBQr9EUkLxVk0SYF3OLubWY2E2g1s/XuvnVIm4eB+93dzWw58DNgSXDubuAr7r7ezMqBKb25jZnRnKjh8W2duDtmFnZJIiI5M+aVvrvvc/e24HE30A7UjWhz1AdnPssABzCzZUCBu68f0u54DuufFKsS1Rzo7mVH55QvVURkXMY1ph8MzawENoxy7mozexZ4APhYcPj1wCEzW2dmG83sv5lZ/MxKnnyrtV5fRPJU1qEfDM2sBW529yMjz7v7fe6+BLgKuC04XAC8BfgscDHQAHxklNe+MZgLaOno6Bj3m8i1xbXlzC4v4gmN64tInskq9M2skEzg3+Pu607X1t0fBRab2WxgD7DR3be5ewr4OdA4ynPWuHuTuzfV1taO+03kmpmxKlGtyVwRyTvZrN4x4C6g3d3vOEWb84J2mFkjUAR0Ak8CVWY2kOSXAVtHe42ppjlRw95DPezu0ri+iOSPbFbvXApcD2w2s03BsVuBhQDufidwLXCDmSWBHuC6YGK338w+CzwcdAqtwHdy/B4mxcn75m7vYkF1acjViIjkxpih7+6PAaddt+jutwO3n+LcemD5hKoL0fnnzGRWaSFPbO/kjy6aH3Y5IiI5oW/knkIsZlxcr3F9EckvCv3TaE5Us7PzOK8cPhF2KSIiOaHQP43BfXi0Xl9E8oNC/zSWzatgZnGB7psrInlDoX8a8ZjRVF/FE7rSF5E8odAfQ3NDDS91HKOjuzfsUkREzphCfwzNwXp9bckgIvlAoT+GC+oqKS2KazJXRPKCQn8MhfEYFy2qYoMmc0UkDyj0s9CcqOa5/d0cPNYXdikiImdEoZ+F5obMev0nduhqX0SmN4V+FpbPr6S4IKYhHhGZ9hT6WSguiLNy4SxN5orItKfQz1Jzooat+45w5EQy7FJERCZMoZ+l5oZq3KFF4/oiMo0p9LPUuLCKwrhpXF9EpjWFfpZKCuOsmD+Lx/XNXBGZxhT649DcUM0zew9ztDcVdikiIhOi0B+H5kQN/WmnbefBsEsREZkQhf44XLSoinjMtHRTRKYthf44lBUX8Ma6Sk3misi0pdAfp+ZENU/tOURPX3/YpYiIjJtCf5yaG6pJ9jsbd2lcX0SmH4X+ODXVVxMz2KClmyIyDSn0x6mipJBl8yo0mSsi05JCfwKaEzVs3HWI3pTG9UVkelHoT8CqRDW9qTRP7T4cdikiIuOi0J+AVfWZm6Vv2KYhHhGZXhT6E1BVVsSSuTN1Jy0RmXYU+hPUnKimdedBkv3psEsREcmaQn+CViVqON7Xz+a9GtcXkelDoT9BqxID4/oa4hGR6WPM0DezBWb2iJm1m9kWM7tplDZXmtnTZrbJzFrM7M0jzleY2V4z+1Yuiw9T7cxiFteW8YTW64vINJLNlX4KuMXdlwKrgU+Z2bIRbR4GVrj7hcDHgO+OOH8b8P/OtNipprmhhpYdB+lPe9iliIhkZczQd/d97t4WPO4G2oG6EW2OuvtA8pUBJ1PQzC4CzgF+nauip4rmRDXdvSm2vnwk7FJERLIyrjF9M6sHVgIbRjl3tZk9CzxA5mofM4sB/x34yzMtdCpqTtQAaEsGEZk2sg59MysH1gI3u/trLm3d/T53XwJcRWY4B+BPgQfdffcYr31jMBfQ0tHRkX31IZtbWcKimlIe12SuiEwTBdk0MrNCMoF/j7uvO11bd3/UzBab2WzgTcBbzOxPgXKgyMyOuvvnRjxnDbAGoKmpaVoNkDcnqnloy37SaScWs7DLERE5rWxW7xhwF9Du7necos15QTvMrBEoAjrd/YPuvtDd64HPAnePDPzprjlRw+GeJM/t7w67FBGRMWVzpX8pcD2w2cw2BcduBRYCuPudwLXADWaWBHqA64ZM7Oa15obBfXiWnlsRcjUiIqc3Zui7+2PAacct3P124PYx2vwA+ME4apsW5leVUjdrBhu2d/GRSxNhlyMiclr6Rm4ONCeqeWJ7FxH540ZEpjGFfg40N1TTeayPlzqOhl2KiMhpKfRzYGC9vpZuishUp9DPgUU1pcyZWaybpYvIlKfQzwEzo7mhhg3bOjWuLyJTmkI/R5oT1Rzo7mVn5/GwSxEROSWFfo6sHlivr314RGQKU+jnyOLacmrKinRTFRGZ0hT6OWJmrEpUazJXRKY0hX4ONSeq2Xuoh91dGtcXkalJoZ9DzQ2Z9fpP6GpfRKYohX4OnX/OTGaVFmoyV0SmLIV+DsVixsX1GtcXkalLoZ9jzYlqdnYeZ89BjeuLyNSj0M+xy5eeQ1E8xl/d+zSp/nTY5YiIDKPQz7HE7DK+cvUF/PalTr7yYHvY5YiIDJPVPXJlfN7btICt+47w/X/fwdJzK3hf04KwSxIRAXSlP2m+8O6lXLK4hi/e9wxtuw6GXY6ICKDQnzQF8Rjf/kAj51QW88kftbL/yImwSxIRUehPpqqyIr5zQxNHe1N84ketnEj2h12SiEScQn+SLZlbwR3vW8Gm3Yf4wn3PaL99EQmVQv8suOKCc/nzy1/H2rY9fP/fd4RdjohEmEL/LLn58tfxe8vO4SsPtvPYC6+GXY6IRJRC/yyJxYx/uO5CFteW8an/2cbOzmNhlyQiEaTQP4vKiwv4zg1NANx4dyvHelMhVyQiUaPQP8sW1ZTxrQ+s5IUD3fzFzzaRTmtiV0TOHoV+CN7yulpuffdSHtqyn2/+64thlyMiEaLQD8l/enOCa1bW8Q//93ke2vJK2OWISEQo9ENiZvzdNW9kxfxK/uKnm3jule6wSxKRCFDoh6ikMM4/X99EaXEBH7+7hUPH+8IuSUTynEI/ZHMrS7jzQxfxyuETfPonG7UHv4hMKoX+FHDRoir+9qoL+LcXXuWrv3w27HJEJI+NGfpmtsDMHjGzdjPbYmY3jdLmSjN72sw2mVmLmb05OH6hmf0ueN7TZnbdZLyJfPC+ixfwkUvq+e5j21nbuifsckQkT2VzE5UUcIu7t5nZTKDVzNa7+9YhbR4G7nd3N7PlwM+AJcBx4AZ3f8HM5gXPfcjdD+X6jeSDL7xnKc+90s3n79vM4jnlXLhgVtgliUieGfNK3933uXtb8LgbaAfqRrQ56oPbR5YBHhx/3t1fCB6/DBwAanNXfn4pjMf49gcbmTOzmE/8qIUD2oNfRHJsXGP6ZlYPrAQ2jHLuajN7FngA+Ngo51cBRcBLEyk0KqrLilhzfRNHelJ84set9Ka0B7+I5E7WoW9m5cBa4GZ3PzLyvLvf5+5LgKuA20Y891zgR8BH3f01y1PM7MZgLqClo6NjvO8h7yybV8HX3ruCjbsO8Tc/36I9+EUkZ7IKfTMrJBP497j7utO1dfdHgcVmNjt4bgWZq/8vuvvjp3jOGndvcvem2lqN/gC8Z/m5fPqy8/hpy27u/t3OsMsRkTyRzeodA+4C2t39jlO0OS9oh5k1khnG6TSzIuA+4G53/1+5KzsaPvP21/P2pXP48i+28tuXtAe/iJy5bK70LwWuBy4LlmRuMrN3m9knzeyTQZtrgWfMbBPwbeC6YGL3fcBbgY8Mee6Fk/FG8tHAHvyJ2WV86p42dncdD7skEZnmbKqNFzc1NXlLS0vYZUwp2189xpXfeox5s2aw9k8uoaw4m5W2IhIlZtbq7k1jtdM3cqeBxOwyvvmBRp7f381f3vuUJnZFZMIU+tPE215fy+fetYQHN7/Ctx/RHvwiMjEK/Wnk429p4KoL5/G1Xz/P+q37wy5HRKYhhf40YmZ89drlvLGuks/8dBMv7Nce/CIyPgr9aaakMM6aGy6ipDDOx+9u4fDxZNglicg0otCfhs6tnMGdH2pk76Ee/uwnbfTr5uoikiWF/jTVVF/Nl6/M7MH/97/SHvwikh0t+J7G3r9qIVtfPsI/P7qNpedWcNXKurGfJCKRpiv9ae5v/mAZqxLV/PXap3l6j25TICKnp9Cf5grjMf7pg43MLi/mEz9q5UC39uAXkVNT6OeBmvJi1txwEQeP9/EnP27THvwickoK/TzxhnmVfO29K2jdeZD/cr/24BeR0WkiN4/8/vJ5bH35CP/jNy+xbF4l169eFHZJIjLF6Eo/z9zyjvO5bMkc/uv9W3h8W2fY5YjIFKPQzzPxmPH1P76QhTWl/OcftvCl//0MT+85pOEeEQE0vJOXKkoK+eFHV/HVXz3LT57czQ9/t5Pz5pRzTWMdV6+s49zKGWGXKCIh0U1U8tzhniQPbt7HurY9PLnjIGZw6eLZXNNYxxUXzKW0SP2+SD7I9iYqCv0I2dl5jHVte1m3cQ+7u3ooLYrzrgvO5drGOlY31BCLWdglisgEKfTllNydlp0HWdu6hwee3kd3b4p5lSVc3VjHNY3zWVxbHnaJIjJOCn3JyolkP+u37mdt2x4efb6DtMOKBbO4trGOP1g+j6qyorBLFJEsKPRl3A50n+D+TS9zb+senn2lm8K4cdmSOVzTOJ//eP4cigq02EtkqlLoyxnZ+vIR1rXt4eebXubVo71UlRbyhyvmcU3jfJbPr8RM4/8iU4lCX3Ii1Z/m3158lbWte/j11v30pdInl39edWEd82Zp+afIVKDQl5w73JPkl5v3sXbI8s9LFtdwbeN83vmGuZQVa/mnSFgU+jKpdnYe476Ne1nXtpddXccpLYpzxQVzubZxPm/S8k+Rs06hL2fFwPLPdW17+MVTg8s/r1qZWf553hwt/xQ5GxT6ctYNLP9c17aHR194lf60s2LBLC47fw61M4upLiuiprwo87OsiIqSQv1FIJIjCn0J1cDyz7Vte2nfd2TUNvGYUVWa6QCqy4qoLh98nPk5vKOoKi0irk5CZFQKfZkyelP9HDyWpPNYL13H+ug61kfn0eDnsT66jvUO+/1wT3LU1zGDWTMKg06h+DUdxdDjNeWZTkLfLZCoyDb0tdxCJl1xQZy5lXHmVpZk1T7Zn+bg8Uwn0HV0oGMY7CAGOo2XOo7y5I4+Dh7vI32Ka5eZJQUnO4WSwjju4HjwMzMnMewxZF4reOwO6RFt4LXHBtoOe4yTTmfqGHq8tChOxYzCzD8lBVQGjytnFFJREvycUXDy94F2BXF1YHLmFPoy5RTGY8yZWcKcmdl1Ev1p53BP8jV/MXSN6Cx6k2nMwDAwiBmYxTLHDGLBF87MDGPw2MBjsOBY5jUGnjf4+LXPY8j5gZGp4339HO5Jcrgnye6u4xwJHqdO1XMFyoriJzuIihEdxODjgc6jgMrSwTalRfFxfaHO3elNpelNpjmR6udEsp8TyTS9qczPzO/9nEhlHveOdn5Y28y5ka+X6k9TVBCjpDBOcWGckuBxSWHws2DwcXFwfEZhfLBNQTw499r2Q9to7miQQl+mvXjMTg7vnDcn7Gomxt3pSfZzpCfF4Z4kR04kOXw88zPTKaQyx3oyvx85kWTvoR7a9x3hSE+S7t7UaV8/HrNhf1XMLCkg2e8nw3ogiHsHQjyVZqIjvzFjMHQLhgd2cUEs81dXEM4F8Rh9qfSwDuTQ8b5hNQ10HL2p9MQKAoriscGOYUhncbJDKYhTVGAUxWMUFcQoDH6O/H3wuA3+fpq2xcN+t5PtwvxGu0JfZAowM0qLCigtKsh6GGyo/rTTfSLJkRGdw8kOpGf4ue4TKQpixqzSomFX1YNXzLEgqEdcUY+8Ei+MUTwkQIsL4hTGbVJCLZ32k53SiZF/cQSdRG9y9L9EBjuOfnr6hnd03SdSvJrqI9mfpi+VPvmzb8jPXE99FsYzHUxh0AkMdBBvqKvkm+9fmdt/2Qhjhr6ZLQDuBuYCaWCNu39jRJsrgduC8yngZnd/LDj3YeCLQdO/dfcf5q58EYHMlfys0iJmlebvrqixmDGjKM6MovhZ/3en+tMk+32wM+hPkxzRMQz8Pthp+LBOJNmfpndopzLwuD9NX8rp60+zsHrytzXJ5ko/Bdzi7m1mNhNoNbP17r51SJuHgfvd3c1sOfAzYImZVQNfAprIzHm1mtn97n4wx+9DRGTSFMRjFMQJpcPJtTGXA7j7PndvCx53A+1A3Yg2R31w7WcZmYAHeCew3t27gqBfD1yRq+JFRGR8xrUGzMzqgZXAhlHOXW1mzwIPAB8LDtcBu4c028OIDiN47o1m1mJmLR0dHeMpSURExiHr0DezcmAtmfH613zF0t3vc/clwFVkxvcBRpvNec2UiLuvcfcmd2+qra3NtiQRERmnrELfzArJBP497r7udG3d/VFgsZnNJnNlv2DI6fnAyxOsVUREztCYoW+ZtVd3Ae3ufscp2pwXtMPMGoEioBN4CHiHmVWZWRXwjuCYiIiEIJvVO5cC1wObzWxTcOxWYCGAu98JXAvcYGZJoAe4LpjY7TKz24Ang+d92d27cvkGREQke9pwTUQkD2S74Zp2cBIRiZApd6VvZh3AzjN4idnAqzkqZ7rTZzGcPo/h9HkMyofPYpG7j7n8ccqF/pkys5Zs/sSJAn0Ww+nzGE6fx6AofRYa3hERiRCFvohIhORj6K8Ju4ApRJ/FcPo8htPnMSgyn0XejemLiMip5eOVvoiInELehL6ZXWFmz5nZi2b2ubDrCZOZLTCzR8ys3cy2mNlNYdcUNjOLm9lGM/tF2LWEzcxmmdm9ZvZs8N/Im8KuKUxm9png/5NnzOwnZjb+W5dNI3kR+mYWB74NvAtYBrzfzJaFW1WoBm58sxRYDXwq4p8HwE1k7gUh8A3gV8GuuCuI8OdiZnXAnwNN7n4BEAf+ONyqJldehD6wCnjR3be5ex/wL8CVIdcUmmxufBMlZjYfeA/w3bBrCZuZVQBvJbOJIu7e5+6Hwq0qdAXADDMrAErJ852A8yX0s7pZSxSd7sY3EfJ14K/I3MM56hqADuD7wXDXd82sLOyiwuLue4GvAbuAfcBhd/91uFVNrnwJ/axu1hI1Y934JgrM7PeBA+7eGnYtU0QB0Aj8k7uvBI4BkZ0DC7Z8vxJIAPOAMjP7ULhVTa58CX3drGWE8dz4Js9dCvyhme0gM+x3mZn9ONySQrUH2OPuA3/53UumE4iqtwPb3b3D3ZPAOuCSkGuaVPkS+k8CrzOzhJkVkZmIuT/kmkKTzY1vosLdP+/u8929nsx/F//q7nl9JXc67v4KsNvMzg8OXQ5sDbGksO0CVptZafD/zeXk+cR2NjdRmfLcPWVmf0bmrlxx4HvuviXkssI06o1v3P3BEGuSqePTwD3BBdI24KMh1xMad99gZvcCbWRWvW0kz7+dq2/kiohESL4M74iISBYU+iIiEaLQFxGJEIW+iEiEKPRFRCJEoS8iEiEKfRGRCFHoi4hEyP8HAo5y9X4uwZMAAAAASUVORK5CYII=\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('val_X.shape',val_X.shape)\npred = classifier.predict(val_X)\nprint('val_X[10]',val_y[:10])\nprint('pred[10]', pred[:10])\naccuracy = multiclass_accuracy(pred, val_y)\nprint(\"Accuracy: \", accuracy)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('val_X.shape', (1000, 3073))\n('val_X[10]', array([2, 3, 4, 3, 5, 5, 3, 6, 1, 7], dtype=uint8))\n('pred[10]', array([1, 1, 3, 1, 3, 2, 3, 3, 3, 4]))\n('Accuracy: ', 0.169)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's check how it performs on validation set\npred = classifier.predict(val_X)\naccuracy = multiclass_accuracy(pred, val_y)\nprint(\"Accuracy: \", accuracy)\n\n# Now, let's train more and see if it performs better\nclassifier.fit(train_X, train_y, epochs=20, learning_rate=1e-3, batch_size=300, reg=1e1)\npred = classifier.predict(val_X)\naccuracy = multiclass_accuracy(pred, val_y)\nprint(\"Accuracy after training: \", accuracy)",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('Accuracy: ', 0.168)\nEpoch 0, loss: 2.301072\nEpoch 1, loss: 2.301391\nEpoch 2, loss: 2.302539\nEpoch 3, loss: 2.301473\nEpoch 4, loss: 2.300723\nEpoch 5, loss: 2.301839\nEpoch 6, loss: 2.302919\nEpoch 7, loss: 2.302318\nEpoch 8, loss: 2.302858\nEpoch 9, loss: 2.301858\nEpoch 10, loss: 2.301810\nEpoch 11, loss: 2.302400\nEpoch 12, loss: 2.301677\nEpoch 13, loss: 2.303707\nEpoch 14, loss: 2.301894\nEpoch 15, loss: 2.302791\nEpoch 16, loss: 2.301297\nEpoch 17, loss: 2.302651\nEpoch 18, loss: 2.302724\nEpoch 19, loss: 2.302103\n('Accuracy after training: ', 0.159)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n\nВ этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n\nТеперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \nДобейтесь точности более чем **20%** на проверочных данных (validation data)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_epochs = 200\nbatch_size = 300\n\nlearning_rates = [1e-3, 1e-4, 1e-5]\nreg_strengths = [1e-4, 1e-5, 1e-6]\n\nbest_classifier = None\nbest_val_accuracy = None\nprint('train_X.shape', train_X.shape)\nprint('train_y.shape',train_y.shape)\nprint('val_X.shape',val_X.shape)\nprint('val_y.shape',val_y.shape)\n\nW= 0.001 * np.random.randn(num_features, num_classes)\nclassifier1 = linear_classifer.LinearSoftmaxClassifier()\nclassifier1.W=W\n\nfor lr in learning_rates:\n    classifier1.W=W\n    loss_history=classifier1.fit(train_X, train_y, epochs=50, learning_rate=lr, batch_size=300, reg=1e1)\n    pred = classifier.predict(val_X)\n    accuracy = multiclass_accuracy(pred, val_y)\n    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n    print('learning rate =', lr,  ' Accuracy',   accuracy)\n\nfor rs in reg_strengths:\n    classifier1.W=W\n    loss_history=classifier1.fit(train_X, train_y, epochs=50, learning_rate=1e-3, batch_size=300, reg=rs)\n    pred = classifier.predict(val_X)\n    accuracy = multiclass_accuracy(pred, val_y)\n    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n    print('learning rate =', lr,  ' Accuracy',   accuracy)",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('train_X.shape', (9000, 3073))\n('train_y.shape', (9000,))\n('val_X.shape', (1000, 3073))\n('val_y.shape', (1000,))\nfirst W = none\nEpoch 0, loss: 2.396022\nEpoch 1, loss: 2.329750\nEpoch 2, loss: 2.310438\nEpoch 3, loss: 2.303819\nEpoch 4, loss: 2.302505\nEpoch 5, loss: 2.302268\nEpoch 6, loss: 2.302367\nEpoch 7, loss: 2.301751\nEpoch 8, loss: 2.302016\nEpoch 9, loss: 2.301596\nEpoch 10, loss: 2.302361\nEpoch 11, loss: 2.301883\nEpoch 12, loss: 2.301978\nEpoch 13, loss: 2.302454\nEpoch 14, loss: 2.302301\nEpoch 15, loss: 2.302720\nEpoch 16, loss: 2.301704\nEpoch 17, loss: 2.302396\nEpoch 18, loss: 2.302453\nEpoch 19, loss: 2.301481\nEpoch 20, loss: 2.301830\nEpoch 21, loss: 2.302437\nEpoch 22, loss: 2.302166\nEpoch 23, loss: 2.302593\nEpoch 24, loss: 2.301051\nEpoch 25, loss: 2.301876\nEpoch 26, loss: 2.301600\nEpoch 27, loss: 2.301571\nEpoch 28, loss: 2.301150\nEpoch 29, loss: 2.302963\nEpoch 30, loss: 2.302537\nEpoch 31, loss: 2.301754\nEpoch 32, loss: 2.302054\nEpoch 33, loss: 2.301162\nEpoch 34, loss: 2.301873\nEpoch 35, loss: 2.302246\nEpoch 36, loss: 2.302968\nEpoch 37, loss: 2.302085\nEpoch 38, loss: 2.302343\nEpoch 39, loss: 2.302014\nEpoch 40, loss: 2.302510\nEpoch 41, loss: 2.302029\nEpoch 42, loss: 2.301997\nEpoch 43, loss: 2.301989\nEpoch 44, loss: 2.302068\nEpoch 45, loss: 2.303432\nEpoch 46, loss: 2.302472\nEpoch 47, loss: 2.303269\nEpoch 48, loss: 2.300724\nEpoch 49, loss: 2.301767\n('learning rate =', 0.001, ' Accuracy', 0.159)\nEpoch 0, loss: 2.572095\nEpoch 1, loss: 2.541844\nEpoch 2, loss: 2.515199\nEpoch 3, loss: 2.490371\nEpoch 4, loss: 2.468964\nEpoch 5, loss: 2.451121\nEpoch 6, loss: 2.433552\nEpoch 7, loss: 2.418391\nEpoch 8, loss: 2.405152\nEpoch 9, loss: 2.392832\nEpoch 10, loss: 2.382571\nEpoch 11, loss: 2.374611\nEpoch 12, loss: 2.366075\nEpoch 13, loss: 2.358548\nEpoch 14, loss: 2.352050\nEpoch 15, loss: 2.346741\nEpoch 16, loss: 2.341285\nEpoch 17, loss: 2.337090\nEpoch 18, loss: 2.332831\nEpoch 19, loss: 2.328682\nEpoch 20, loss: 2.327030\nEpoch 21, loss: 2.323363\nEpoch 22, loss: 2.321175\nEpoch 23, loss: 2.319390\nEpoch 24, loss: 2.317609\nEpoch 25, loss: 2.315201\nEpoch 26, loss: 2.312355\nEpoch 27, loss: 2.312652\nEpoch 28, loss: 2.311645\nEpoch 29, loss: 2.310088\nEpoch 30, loss: 2.309862\nEpoch 31, loss: 2.308199\nEpoch 32, loss: 2.308363\nEpoch 33, loss: 2.307007\nEpoch 34, loss: 2.307130\nEpoch 35, loss: 2.305532\nEpoch 36, loss: 2.306145\nEpoch 37, loss: 2.305179\nEpoch 38, loss: 2.304491\nEpoch 39, loss: 2.304533\nEpoch 40, loss: 2.304271\nEpoch 41, loss: 2.304368\nEpoch 42, loss: 2.303002\nEpoch 43, loss: 2.302726\nEpoch 44, loss: 2.302442\nEpoch 45, loss: 2.302657\nEpoch 46, loss: 2.303682\nEpoch 47, loss: 2.303384\nEpoch 48, loss: 2.302093\nEpoch 49, loss: 2.303498\n('learning rate =', 0.0001, ' Accuracy', 0.159)\nEpoch 0, loss: 2.603135\nEpoch 1, loss: 2.597999\nEpoch 2, loss: 2.594852\nEpoch 3, loss: 2.592301\nEpoch 4, loss: 2.588544\nEpoch 5, loss: 2.585174\nEpoch 6, loss: 2.581271\nEpoch 7, loss: 2.578484\nEpoch 8, loss: 2.574497\nEpoch 9, loss: 2.570973\nEpoch 10, loss: 2.569176\nEpoch 11, loss: 2.564918\nEpoch 12, loss: 2.562345\nEpoch 13, loss: 2.558751\nEpoch 14, loss: 2.554967\nEpoch 15, loss: 2.553020\nEpoch 16, loss: 2.550089\nEpoch 17, loss: 2.548045\nEpoch 18, loss: 2.544602\nEpoch 19, loss: 2.541452\nEpoch 20, loss: 2.538691\nEpoch 21, loss: 2.535890\nEpoch 22, loss: 2.533134\nEpoch 23, loss: 2.530286\nEpoch 24, loss: 2.528071\nEpoch 25, loss: 2.524936\nEpoch 26, loss: 2.522012\nEpoch 27, loss: 2.519268\nEpoch 28, loss: 2.516879\nEpoch 29, loss: 2.513850\nEpoch 30, loss: 2.511643\nEpoch 31, loss: 2.508588\nEpoch 32, loss: 2.507056\nEpoch 33, loss: 2.503629\nEpoch 34, loss: 2.501782\nEpoch 35, loss: 2.499527\nEpoch 36, loss: 2.496640\nEpoch 37, loss: 2.495569\nEpoch 38, loss: 2.492169\nEpoch 39, loss: 2.489191\nEpoch 40, loss: 2.487326\nEpoch 41, loss: 2.485572\nEpoch 42, loss: 2.483917\nEpoch 43, loss: 2.481425\nEpoch 44, loss: 2.479492\nEpoch 45, loss: 2.477223\nEpoch 46, loss: 2.474249\nEpoch 47, loss: 2.473491\nEpoch 48, loss: 2.470358\nEpoch 49, loss: 2.470059\n('learning rate =', 1e-05, ' Accuracy', 0.159)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_epochs = 200\nbatch_size = 300\n\nlearning_rates = [1e-3, 1e-4, 1e-5]\nreg_strengths = [1e-4, 1e-5, 1e-6]\n\nbest_classifier = None\nbest_val_accuracy = None\nprint('train_X.shape', train_X.shape)\nprint('train_y.shape',train_y.shape)\nprint('val_X.shape',val_X.shape)\nprint('val_y.shape',val_y.shape)\n\nW= 0.001 * np.random.randn(num_features, num_classes)\nclassifier1 = linear_classifer.LinearSoftmaxClassifier(W)\nloss_history = classifier1.fit(train_X, train_y, epochs=50, learning_rate=1e-3, batch_size=300, reg=1e1)\n\nfor lr in learning_rates:\n    classifier.fit(train_X, train_y, epochs=2, learning_rate=lr, batch_size=300, reg=1e1)\n    pred = classifier.predict(val_X)\n    accuracy = multiclass_accuracy(pred, val_y)\n    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n    print('learning rate =', lr,  ' Accuracy',   accuracy)\n\n# TODO use validation set to find the best hyperparameters\n# hint: for best results, you might need to try more values for learning rate and regularization strength \n# than provided initially\n\n#print('best validation accuracy achieved: %f' % best_val_accuracy)",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": "('train_X.shape', (9000, 3073))\n('train_y.shape', (9000,))\n('val_X.shape', (1000, 3073))\n('val_y.shape', (1000,))\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__init__() takes exactly 1 argument (2 given)",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5edcb1928827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclassifier1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSoftmaxClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 1 argument (2 given)"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Какой же точности мы добились на тестовых данных?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_pred = best_classifier.predict(test_X)\ntest_accuracy = multiclass_accuracy(test_pred, test_y)\nprint('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}