{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_X.shape initial', (10000, 32, 32, 3))\n",
      "('train_x.mean', 0.446616470588233)\n",
      "('indices initial', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
      "('indices after shuffle', array([6252, 4684, 1731, 4742, 4521, 6340,  576, 5202, 6363,  439]))\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    print('train_x.mean',mean_image[0])\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    #print('train_flat_ones.shape', train_flat_with_ones.shape)\n",
    "    #print('train_flat_ones[0]', train_flat_with_ones[0])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n",
    "print('train_X.shape initial', train_X.shape) #expect 1000,32,32,3\n",
    "#print('train_X[0,0]', train_X[0,0])\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_sum(x):\n",
    "    #print('x.shape',x.shape)\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "probs = linear_classifer.softmax(np.array([1, 1, 1]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0][0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "1\n",
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.zeros(5)\n",
    "print(a)\n",
    "print(np.ndim(a))\n",
    "b=a.reshape(1,-1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('probs', array([[0.57611688, 0.21194156, 0.21194156]]))\n",
      "('loss=', array([1.55144471]))\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "print('probs',probs)\n",
    "#linear_classifer.cross_entropy_loss(probs, 1)\n",
    "loss=linear_classifer.cross_entropy_loss(probs, np.array([[1]]))\n",
    "print('loss=',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss=', 1.551444713932051)\n",
      "('grad', array([ 0.57611688, -0.78805844,  0.21194156]))\n",
      "('it.shape=', (3,))\n",
      "Gradient check passed!\n",
      "----------star check batch------\n",
      "('loss=', 1.051444713932051)\n",
      "('grad', array([[ 0.28805844, -0.39402922,  0.10597078],\n",
      "       [-0.21194156,  0.10597078,  0.10597078]]))\n",
      "('it.shape=', (2, 3))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n",
    "\n",
    "print('----------star check batch------')\n",
    "predictions=np.array([[1,0,0],[1,0,0]],dtype=np.float)\n",
    "target_index=np.array([[1],[0]],dtype=np.int)\n",
    "loss, grad =linear_classifer.softmax_with_cross_entropy_batch(predictions, target_index)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, np.array([1,0]), np.array([[1,0,0],[1,0,0]],np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- batch 2, num 4 ----------\n",
      "('predictions', array([[ 1.,  2., -1.,  1.],\n",
      "       [ 1.,  2., -1., -1.]]))\n",
      "('it.shape=', (2, 4))\n",
      "Gradient check passed!\n",
      "----------- batch 3, num 4 ----------\n",
      "('it.shape=', (3, 4))\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "\n",
    "print('----------- batch 2, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 2\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "print('----------- batch 3, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss', 0.5472365148987376)\n",
      "('dW=', array([[-0.22019927,  0.22019927],\n",
      "       [-0.20332316,  0.20332316],\n",
      "       [ 0.23874859, -0.23874859]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 4\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss',loss)\n",
    "print('dW=',dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('target_index', array([1, 1, 1, 1, 1]))\n",
      "('loss SoftMax', 0.44141919750255204)\n",
      "('dW Softmax=', array([[-0.16896493,  0.16896493],\n",
      "       [-0.15906129,  0.15906129],\n",
      "       [ 0.19099887, -0.19099887]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('loss L2', 0.12)\n",
      "('dW  L2=', array([[ 0.02,  0.04],\n",
      "       [-0.02,  0.02],\n",
      "       [ 0.02,  0.04]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# this is my test\n",
    "batch_size = 5\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print('target_index',target_index)\n",
    "\n",
    "loss_sm, dW_sm = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss SoftMax',loss_sm)\n",
    "print('dW Softmax=',dW_sm)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "loss_l2, dW_l2=linear_classifer.l2_regularization(W, 0.01)\n",
    "print('loss L2',loss_l2)\n",
    "print('dW  L2=',dW_l2)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n",
    "\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, 0.01, X, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, 0.01,X,target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num_train', 900)\n",
      "('num_features', 3073)\n",
      "('num_classe', 10)\n",
      "('batch_size', 20)\n",
      "('sections', array([ 20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240, 260,\n",
      "       280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520,\n",
      "       540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780,\n",
      "       800, 820, 840, 860, 880]))\n",
      "('shuffled indices [0:10]', array([722, 826, 642, 368,  46, 492, 303, 314, 388,  10]))\n",
      "('batches_indices', 45)\n",
      "('batches_indices[0].shape', (20,))\n",
      "('loss', 2.605083910175978)\n",
      "('it.shape=', (3073, 10))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "\n",
    "num_train=train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "batch_size=20\n",
    "reg=1e1\n",
    "learning_rate=1e-3\n",
    "print('num_train', num_train)\n",
    "print('num_features', num_features)\n",
    "print('num_classe',num_classes)\n",
    "print('batch_size',batch_size)\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "sections = np.arange(batch_size, num_train, batch_size)\n",
    "print('sections',sections)\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "print('shuffled indices [0:10]', shuffled_indices[0:10])\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n",
    "print('batches_indices', len(batches_indices))\n",
    "print('batches_indices[0].shape', batches_indices[0].shape)\n",
    "\n",
    "target_index=train_y[batches_indices[0]]\n",
    "X=train_X[batches_indices[0]]\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, reg, X, target_index)\n",
    "print('loss',loss)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, reg,X,target_index), W)\n",
    "#loss_history = classifier.fit(train_X, train_y, epochs=1, learning_rate=1e-3, batch_size=1, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1 3 5]\n",
      "[ 2  6 10]\n",
      "[853 402  76  22 568 763 452 838 542 331  55 187 616  47 181 861 670  66\n",
      " 803 716]\n",
      "[5 9 1 4 0 3 0 0 2 7 2 6 1 5 5 3 2 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "t=np.arange(10)*2\n",
    "i=np.array([1,3,5])\n",
    "print(t)\n",
    "print(i)\n",
    "print(t[i])\n",
    "print(batches_indices[0])\n",
    "print(train_y[batches_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.398027\n",
      "Epoch 1, loss: 2.331343\n",
      "Epoch 2, loss: 2.310226\n",
      "Epoch 3, loss: 2.304392\n",
      "Epoch 4, loss: 2.303214\n",
      "Epoch 5, loss: 2.301560\n",
      "Epoch 6, loss: 2.302410\n",
      "Epoch 7, loss: 2.301187\n",
      "Epoch 8, loss: 2.302606\n",
      "Epoch 9, loss: 2.301999\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc0b1ad1110>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi41LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvSM8oowAAHnNJREFUeJzt3XtwXOWZ5/HvI6m7ZUnWXTa+IglzM8TYRiAlwDCTbIBcDUUm5AbJZHcYEpLAFOzksqlKzZCaWiYJlWRyISxkZxMYMiyXQEImgbDeYZmJBfId2wSwDdjY2LJs2bpYal2e/aOPZFnIVsuWfNTn/D5Vrm6dflt+usv+vaef8/Y55u6IiEg85IVdgIiInDoKfRGRGFHoi4jEiEJfRCRGFPoiIjGi0BcRiRGFvohIjCj0RURiRKEvIhIjBWEXMFp1dbXX1taGXYaISE5ZvXr1PnevGW/ctAv92tpaWlpawi5DRCSnmNnr2YxTe0dEJEbGDX0zW2BmK81ss5ltMrNbjjP2IjPrN7OPjNj2aTN7Jfjz6ckqXEREJi6b9k4/cJu7rzGzmcBqM3va3TePHGRm+cCdwFMjtlUC3wAaAA+e+4S7H5i0VyAiIlkbd0/f3Xe7+5rgfgewBZg3xtAvAo8Ae0dsuxJ42t33B0H/NHDVSVctIiInZEI9fTOrBZYBzaO2zwOuAX486inzgB0jft7JGBOGmd1oZi1m1tLa2jqRkkREZAKyDn0zKyGzJ3+rux8a9fB3gS+7++CJFOHu97h7g7s31NSMu+JIREROUFZLNs0sQSbwH3D3R8cY0gD8wswAqoH3m1k/8CbwpyPGzQf+70nUKyIiJyGb1TsG3Adscfe7xhrj7nXuXuvutcDDwOfd/ZfA74ArzKzCzCqAK4Jtk669O833fv8KL755cCp+vYhIJGSzp38JcD2w0czWBdu+BiwEcPe7j/VEd99vZncALwSb/s7d959EvceUl2d875mXGXDn/HllU/FXiIjkvHFD392fAyzbX+junxn180+Bn064sgkqLUxw3twymre1TfVfJSKSsyL1jdym+krW7minp28g7FJERKalSIV+Y10V6f5B1r7RHnYpIiLTUqRC/6K6SsygebtaPCIiY4lU6JfNSHDe3FJWqa8vIjKmSIU+ZFo8a99QX19EZCyRC/2m+ip6+wdZv0N9fRGR0SIX+hfXZvr6q7ZNydcBRERyWuRCv6wowbmnlepgrojIGCIX+pBp8ax+/QC9/erri4iMFMnQb6yvpLd/kA07dR4eEZGRohn6wXr9VVvV4hERGSmSoV9elOTs2TNZpb6+iMhRIhn6cKSvn+4/oeu6iIhEUqRDv6dvkA07tV5fRGRIZEP/4rpKAJq3a72+iMiQyIZ+ZXGSc06bqfPwiIiMENnQh8wqnpbXDtA3oL6+iAhEPPSb6qs43Deg9foiIoFIh/5QX18tHhGRjEiHflVJirNml+hgrohIINKhD5kWT8tr+9XXFxEhBqHfWFdFd3qAjW+qry8iEv3Qrw/W6+v8+iIi0Q/96pIUZ84q0cFcERFiEPqQ2dtveW0//erri0jMxSL0m+qr6EoP8OKuQ2GXIiISqliEvtbri4hkxCL0Z80s5IyaYpoV+iISc7EIfci0eF547YD6+iISa7EJ/cb6Kjp7+9m8W319EYmv2IR+k/r6IiLjh76ZLTCzlWa22cw2mdktY4xZYWYbzGydmbWY2aUjHvuH4HlbzOz7ZmaT/SKyMau0kPrqYlbpS1oiEmPZ7On3A7e5+2KgCbjZzBaPGvMMcIG7LwU+C9wLYGbvAi4BlgDnAxcBl09S7RPWWF/FC9v3MzDoYZUgIhKqcUPf3Xe7+5rgfgewBZg3akynuw8laTEwdN+BQiAJpIAEsGdySp+4pvpKOnr72az1+iISUxPq6ZtZLbAMaB7jsWvM7CXgSTJ7+7j7H4CVwO7gz+/cfcvJlXzimuqrAPX1RSS+sg59MysBHgFudfe37Sq7+2Pufg5wNXBH8JxFwLnAfDKfDt5tZpeN8btvDI4FtLS2tp7YK8nC7NJC6qqLad6u0BeReMoq9M0sQSbwH3D3R4831t2fBerNrBq4BlgVtH86gX8F3jnGc+5x9wZ3b6ipqZnwi5iIxrpKmtXXF5GYymb1jgH3AVvc/a5jjFk0tCrHzJaT6d+3AW8Al5tZQTBxXE7mmEBomuqr6OjpZ4vW64tIDBVkMeYS4Hpgo5mtC7Z9DVgI4O53A9cCN5hZH3AYuM7d3cweBt4NbCRzUPe37v6rSX4NEzJ0fv1V29o4f15ZmKWIiJxy44a+uz8HHHdtvbvfCdw5xvYB4K9OuLopMKdsBqdXFbFq237+y2X1YZcjInJKxeYbuSM11VXxwmv7GVRfX0RiJp6hf0YlBw/3seUt9fVFJF5iGfqNdZn1+rpurojETSxDf275DBZWFulLWiISO7EMfcis139efX0RiZnYhn5TfRXt3X38cU9H2KWIiJwysQ39kev1RUTiIrahP7+iiPkVM3QwV0RiJbahD5kWT/P2NvX1RSQ2Yh36jXWVHOju4+W96uuLSDzEOvSHzq+vFo+IxEWsQ39BZRHzymfoYK6IxEasQx8yq3iat+/nyNUeRUSiK/ah31Rfxf6uNK/s7Qy7FBGRKafQr9N1c0UkPmIf+gsqZzC3rFAHc0UkFmIf+mY2vF5ffX0RibrYhz5kDubu60yztVV9fRGJNoU+R9br/0EtHhGJOIU+sLCyiNNKC3UwV0QiT6HPUF+/kuZtWq8vItGm0A801Vexr7OXra1dYZciIjJlFPqBxqHz8GxXi0dEokuhH6itKmJ2aYpVOpgrIhGm0A+YGY11VazapvX6IhJdCv0RmuqraO3oZfs+9fVFJJoU+iM0DV83Vy0eEYkmhf4IddXF1MxM6WCuiESWQn+EofPwqK8vIlGl0B+lsa6SPYd6ea2tO+xSREQmnUJ/lCPXzVWLR0SiR6E/yhk1xVSXpHQeHhGJpHFD38wWmNlKM9tsZpvM7JYxxqwwsw1mts7MWszs0hGPLTSzp8xsS/A7aif3JUwuM6OxvpJVOg+PiERQNnv6/cBt7r4YaAJuNrPFo8Y8A1zg7kuBzwL3jnjsZ8C33P1c4GJg78mXPbWa6qt461APb+xXX19EomXc0Hf33e6+JrjfAWwB5o0a0+lHdouLAQcIJocCd396xLhpn6RNdUPr9dXiEZFomVBPP2jNLAOax3jsGjN7CXiSzN4+wFlAu5k9amZrzexbZpZ/ciVPvUWzSqgqTuq6uSISOVmHvpmVAI8At7r7odGPu/tj7n4OcDVwR7C5ALgMuB24CKgHPjPG774xOBbQ0traOuEXMdm0Xl9Eoiqr0DezBJnAf8DdHz3eWHd/Fqg3s2pgJ7DO3be5ez/wS2D5GM+5x90b3L2hpqZmwi9iKjTWV7LrYA879h8OuxQRkUmTzeodA+4Dtrj7XccYsygYh5ktB1JAG/ACUG5mQ0n+bmDzZBQ+1YbW66/SKRlEJEIKshhzCXA9sNHM1gXbvgYsBHD3u4FrgRvMrA84DFwXHNgdMLPbgWeCSWE18D8m+TVMiTNnlVBZnGTVtjY+2rAg7HJERCbFuKHv7s8BNs6YO4E7j/HY08CSE6ouRJnz61fqYK6IRIq+kXscTfVVvNl+mB1ary8iEaHQP47Geq3XF5FoUegfx1mzZlJRlKB5u1o8IhINCv3jyMs7ct1cEZEoUOiPo7G+kp0HDrPzgPr6IpL7FPrjOHJ+fbV4RCT3KfTHcfbsmZQXJdTiEZFIUOiPIy/PuLi2UgdzRSQSFPpZaKqv4o393exq13l4RCS3KfSzMLRev1nn4RGRHKfQz8K5p5VSNiPBqq1q8YhIblPoZyEvz7iotlJn3BSRnKfQz1JTfSWvt3Wz+6D6+iKSuxT6WdJ6fRGJAoV+ls6dU8rMwgIdzBWRnKbQz1J+Xub8+qu0py8iOUyhPwGNdVVs39fFnkM9YZciInJCFPoTMHzdXJ2SQURylEJ/AhbPLWVmqkAtHhHJWQr9CcjPMy6qq9TBXBHJWQr9CWqqr2Rbaxd71dcXkRyk0J+gxrqgr6+zbopIDlLoT9B5c0spSRXQrIO5IpKDFPoTVJCfx0W1FVrBIyI5SaF/Ahrrq9ja2kVrR2/YpYiITIhC/wQMn4dHq3hEJMco9E/A+XNLKU7mq8UjIjlHoX8CCvLzaKit1Bk3RSTnKPRPUFN9Fa/s7WRfp/r6IpI7FPonaOi6uc9rvb6I5BCF/gl6x7wyitTXF5Eco9A/QYn8PC48Xev1RSS3jBv6ZrbAzFaa2WYz22Rmt4wxZoWZbTCzdWbWYmaXjnq81Mx2mtkPJrP4sDXVV/Hynk7a1NcXkRyRzZ5+P3Cbuy8GmoCbzWzxqDHPABe4+1Lgs8C9ox6/A3j2ZIudbobW66uvLyK5YtzQd/fd7r4muN8BbAHmjRrT6e4e/FgMDN3HzC4EZgNPTVbR08WS+WXMSKivLyK5Y0I9fTOrBZYBzWM8do2ZvQQ8SWZvHzPLA74D3H6yhU5Hifw8GmoraNaevojkiKxD38xKgEeAW9390OjH3f0xdz8HuJpMOwfg88Bv3H3nOL/7xuBYQEtra2v21U8DTfVVvPRWB/u70mGXIiIyrqxC38wSZAL/AXd/9Hhj3f1ZoN7MqoF3Al8ws9eAbwM3mNl/H+M597h7g7s31NTUTPQ1hKqxTuv1RSR3ZLN6x4D7gC3uftcxxiwKxmFmy4EU0Obun3T3he5eS6bF8zN3/8qkVT8NLJlfTmEiT319EckJBVmMuQS4HthoZuuCbV8DFgK4+93AtWT24vuAw8B1Iw7sRlqyQOv1RSR3jBv67v4cYOOMuRO4c5wx/wT80wRqyxlNdVXc9fuXae9OU16UDLscEZFj0jdyJ0HTGVW4o1U8IjLtKfQnwZL5ZaQK8nSqZRGZ9hT6kyBVkK++vojkBIX+JGmsq2LLW4c42N0XdikiIsek0J8kTfWVuMPzr6nFIyLTl0J/klywoJxUgdbri8j0ptCfJIWJfJYtLKd5u0JfRKYvhf4kaqqvYtOuQxw8rL6+iExPCv1J1FiXWa//gtbri8g0pdCfRMsWlpMsyFOLR0SmLYX+JCpM5LNsQTmr9CUtEZmmFPqTrLG+ik27DnKoR319EZl+FPqT7PKzqhl0+P7vXwm7FBGRt1HoT7ILT6/khneezr3PbefXG3aFXY6IyFEU+lPg6x9YzIWnV/A3D2/g5T0dYZcjIjJMoT8FkgV5/OiTyylKFnDTz1ervy8i04ZCf4rMLi3kB59Yxuv7u7n9ofUMDsbiQmIiMs0p9KdQU30VX33fOTy1eQ8//retYZcjIqLQn2r/+dI6PrhkDt956o/8v1dawy5HRGJOoT/FzIw7r13ColklfOnBtew80B12SSISYwr9U6A4VcBPrm+gf8D53P1r6OkbCLskEYkphf4pUlddzF3XLWXjmwf5xuObwi5HRGJKoX8KvXfxbL7wZ4v4l5YdPPj8G2GXIyIxpNA/xf76vWdx2ZnVfOPxTazb0R52OSISMwr9Uyw/z/j+x5ZRMzPF5+9fTVtnb9gliUiMKPRDUFGc5CfXX8i+rjRffHAt/QODYZckIjGh0A/J+fPK+ObV5/MfW9v49lMvh12OiMSEQj9EH21YwCcaF3L3v23lty/uDrscEYkBhX7IvvGhxSxdUM5tD63n1b2dYZcjIhGn0A9ZqiCfH39qOYWJfP7q5y109vaHXZKIRJhCfxqYUzaDf/zEMrbv6+K//u/1uOuMnCIyNRT608S7zqjmK+87h3998S3ueXZb2OWISESNG/pmtsDMVprZZjPbZGa3jDFmhZltMLN1ZtZiZpcG25ea2R+C520ws+um4kVExV9eVs/733Ead/72Jf7j1X1hlyMiEZTNnn4/cJu7LwaagJvNbPGoMc8AF7j7UuCzwL3B9m7gBnc/D7gK+K6ZlU9O6dFjZvzDRy6gvqaELz64ll3th8MuSUQiZtzQd/fd7r4muN8BbAHmjRrT6Uca0cWAB9tfdvdXgvu7gL1AzeSVHz0lqQLu/tSF9PYP8rkH1tDbrzNyisjkmVBP38xqgWVA8xiPXWNmLwFPktnbH/34xUAS0CWkxrFoVgnf/vMlrN/Rzt/+anPY5YhIhGQd+mZWAjwC3Oruh0Y/7u6Pufs5wNXAHaOeOwf4OfAX7v62cw6Y2Y3BsYCW1lZdXQrgqvPncNPlZ/DPzW/wUMuOsMsRkYjIKvTNLEEm8B9w90ePN9bdnwXqzaw6eG4pmb3//+buq47xnHvcvcHdG2pq1P0ZcvsVZ3HJoiq+/ssX2bjzYNjliEgEZLN6x4D7gC3uftcxxiwKxmFmy4EU0GZmSeAx4Gfu/vDklR0PBfl5fP9jy6guTnLT/as50JUOuyQRyXHZ7OlfAlwPvDtYkrnOzN5vZjeZ2U3BmGuBF81sHfBD4LrgwO5HgT8BPjPiuUun4oVEVVVJih9/6kJaO3r50i/WMjCoL26JyImz6fbtz4aGBm9paQm7jGnnweff4KuPbuQLf7aI2688O+xyRGSaMbPV7t4w3jh9IzdHfPzihVzXsIAfrHyVpza9FXY5IpKjFPo55G9XnMeS+WXc9tB6trXqjJwiMnEK/RxSmMjnR59cTkG+cdP9q+nSGTlFZIIU+jlmfkUR//jx5by6t5MvP7JBZ+QUkQlR6OegS8+s5vYrz+bXG3bz039/LexyRCSHKPRz1OcuP4Mrz5vN3/9mC83b2sIuR0RyhEI/R5kZ3/7zCzi9soib/3ktew71hF2SiOQAhX4Om1mY4CfXX0h3up/P3b+adP/bTmskInIUhX6OO3P2TL71kQtY80Y733xSZ+QUkeNT6EfAB5bM4S8vq+Nnf3idR9fsDLscEZnGFPoR8eWrzqGpvpKvPrqRTbt0Rk4RGZtCPyIK8vP4wSeWU1GU5HP3r+Fgd1/YJYnINKTQj5DqkhQ/+tRydh88zK3/spZBnZFTREZR6EfM8oUVfOND57Hyj6185+k/6lTMInKUgrALkMn3ycaFrN/Rzg9XbuWhlp18aMlcViydy5L5ZQTXuhGRmNL59COqf2CQpzbv4fF1b7LypVbSA4PUVhXx4aXzWLF0LmfUlIRdoohMomzPp6/Qj4GDh/v47Yu7eXzdLv6wrQ13eMe8MlYsncsHl8zltLLCsEsUkZOk0Jcx7TnUw6/W7+KJ9bvYsPMgZtBUV8WKpXN53/lzKCtKhF2iiJwAhb6Ma1trJ0+s38Xj63axfV8XiXzjT8+exdVL5/Gec2dRmMgPu0QRyZJCX7Lm7mx88yCPr9vFr9bvYm9HL8XJfK48/zRWLJ3HJWdUUZCvhV4i05lCX07IwKDTvK2Nx9ft4jcv7qajp5/qkiQfeMccPrx0HssXlmsFkMg0pNCXk9bbP8DKl1p5Yv2b/H7LXtL9gyyonMGKCzIrgM6cPTPsEkUkoNCXSdXR08fvNmWWgP77q/sYdDh3Tikrls7lQxfMZV75jLBLFIk1hb5MmdaOXp7csItfrtvFuh3tAFxcW8mKZXN5//lzqChOhlyhSPwo9OWUeL2tiyfW7eKX695ka2sXBXnG5WfV8OGlc3nv4tkUJfWlb5FTQaEvp5S7s3n3IZ5Yl/kOwO6DPRQl87li8Wz+5KwaKouTlBclKZ+RoLwoQWlhgrw8HRAWmSwKfQnN4KDz/Gv7MyuANu7m4OG3n+bZDMpmJCifkaAsmAwqihKUFyUz24uG/gxNFJnb0hkJ8jVZiLyNQl+mhXT/IDsOdNPe3cfBw2nau/uCP2naDwf3D/dxsDvNgWD7oZ7+Y/4+MygtDCaEURNG2YhPEqMnjNLCAn3XQCIt29BXw1WmVLIgb8IndxsYdA4d7uNAMDEc7O6jPZgwDnRnJojhCaM7zettXbR393Gop4/j7cNUFCWYXVpIzcwUs0sLmTXidtbwbYpUgb6JLNGl0JdpJz/PqChOTngV0MCg09HTN/yJYXjCCD5F7OvsZW9HL3sP9fDq3k72dvSOeb2BiqIEs2YWMqs0xayZhcwuTR2ZIIJtuTI5pPsH6ertp7O3n650f3B/gMPpfopTBVQUJSkvSlBRlKQomR/JL94NDjoHD/fR1tXLvs40bZ3p4fv7u3pp60zTlR4gVZBHYSKfGYmh23xSiXwKE3nMSOQPbytM5JEavn9kW2Hwc2Eij2R+3rR9LxX6Ehn5eZZp6RQlgeJxxw8OOvu70+w51DM8Gew91MuejqHbXl7du4/Wjl76x5gcyosSzJ559EQwO/jUMDvYVjMzNaFzGLk73ekBOoeCevh2YMT9/qMe7+odOHpsOtjW0096YDDrvzuZnzc8AQzdVhQnKJuRpGLk9uLk8PGX8hmJU942G3qP2jrT7AtCu62zl7auI4He1plmX7Btf1d6zMndDCqLklQWJylKFdDbN0Bv/yA9fQMc7hugp2+Anr7s37+R8ozhSSAzeRyZOIYmkdTICaMgnxnJfBZUFPHRixac7Ft0XAp9ia28PKO6JEV1SYrzjjNuaHIYmhBaD/UOTxRDt1v37qO1s5e+gbEnh6FPCjUlKRzo6AkCO310eHel+4/bohpiBsXJAkpSBRSn8oPbAhYUFw1vK04VUJIsoKQw89jQmJJUJnw6e/qDNlnmk9CB7jTtXcFtdx9bWzs58Hrm8bEmvSEzC498YigvGjVBjJxAhsckKEkVHLUnnO4fZH/XyKAeCu6RgR7sqXf1HjOMS1IFVJUkqSpOsqCyiGULy6kqTmW2laSoLs7cVpVk6hlvUYC7j5oIBjmcHqCnf2hSGL1tcMT2Ec/pG6B3xP327r5g29GTzNIF5eGHvpktAH4GzAYcuMfdvzdqzArgDmAQ6Adudffngsc+DXw9GPpNd/9fk1e+yNQbOTkspvSY4wYHnQPd6aMmg70jJoc9h3rZ1tpFXl4msGcWFlAZhFNJ8kggF6eOhHRJamRgHwn3GYn8U7bk1d3pSg9woGvouEp6eGIYfdvenea1fV0c6E7TcZwD8ol8o2xGkuJUPge6jn3wPpmfFwR2kqriFGfMKqG6JEVVcWYPvbrkSKBXFScn/cywZja8x14+qb/57dz9lFzedNzVO2Y2B5jj7mvMbCawGrja3TePGFMCdLm7m9kS4CF3P8fMKoEWoIHMhLEauNDdDxzr79PqHZFo6B8YPPqTRFf6qJ/bu9N09Q5QUZQY3vuuKk5RXXJkb3zmqE8EcmyTtnrH3XcDu4P7HWa2BZgHbB4xpnPEU4rJBDzAlcDT7r4/KOpp4CrgwSxfh4jkqIL8vOFPSDJ9TOgIjJnVAsuA5jEeu8bMXgKeBD4bbJ4H7BgxbGewbfRzbzSzFjNraW1tnUhJIiIyAVmHftDCeYRMv/7Q6Mfd/TF3Pwe4mkx/P2vufo+7N7h7Q01NzUSeKiIiE5BV6JtZgkzgP+Dujx5vrLs/C9SbWTXwJjDyUPT8YJuIiIRg3NC3zFGU+4At7n7XMcYsCsZhZsuBFNAG/A64wswqzKwCuCLYJiIiIchmnf4lwPXARjNbF2z7GrAQwN3vBq4FbjCzPuAwcJ1nlgXtN7M7gBeC5/3d0EFdERE59XTCNRGRCMh2yaZOOygiEiMKfRGRGJl27R0zawVeP4lfUQ3sm6Rycp3ei6Pp/Tia3o8jovBenO7u4655n3ahf7LMrCWbvlYc6L04mt6Po+n9OCJO74XaOyIiMaLQFxGJkSiG/j1hFzCN6L04mt6Po+n9OCI270XkevoiInJsUdzTFxGRY4hM6JvZVWb2RzN71cy+EnY9YTKzBWa20sw2m9kmM7sl7JrCZmb5ZrbWzH4ddi1hM7NyM3vYzF4ysy1m9s6wawqTmf118P/kRTN70MwKw65pKkUi9M0sH/gh8D5gMfBxM1scblWh6gduc/fFQBNwc8zfD4BbgC1hFzFNfA/4bXAq9AuI8ftiZvOALwEN7n4+kA98LNyqplYkQh+4GHjV3be5exr4BbAi5JpC4+673X1NcL+DzH/qt128Ji7MbD7wAeDesGsJm5mVAX9C5sy5uHva3dvDrSp0BcAMMysAioBdIdczpaIS+lldoSuOjne1sxj5LvA3wGDYhUwDdUAr8D+Ddte9ZlYcdlFhcfc3gW8Db5C5LOxBd38q3KqmVlRCX8Yw3tXO4sDMPgjsdffVYdcyTRQAy4Efu/syoAuI7TGw4DofK8hMhnOBYjP7VLhVTa2ohL6u0DXKRK52FnGXAB82s9fItP3ebWb3h1tSqHYCO9196JPfw2Qmgbj6T8B2d2919z7gUeBdIdc0paIS+i8AZ5pZnZklyRyIeSLkmkKTzdXO4sLdv+ru8929lsy/i//j7pHekzsed38L2GFmZweb3gNsDrGksL0BNJlZUfD/5j1E/MB2NlfOmvbcvd/MvkDmUoz5wE/dfVPIZYVpzKuduftvQqxJpo8vAg8EO0jbgL8IuZ7QuHuzmT0MrCGz6m0tEf92rr6RKyISI1Fp74iISBYU+iIiMaLQFxGJEYW+iEiMKPRFRGJEoS8iEiMKfRGRGFHoi4jEyP8HqMmw0UdGavoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val_{.shape', (1000, 3073))\n",
      "('val_X[10]', array([2, 9, 8, 4, 6, 5, 3, 3, 3, 1], dtype=uint8))\n",
      "('pred[10]', array([2, 4, 4, 3, 5, 7, 5, 1, 2, 5]))\n",
      "('Accuracy: ', 0.131)\n"
     ]
    }
   ],
   "source": [
    "print('val_X.shape',val_X.shape)\n",
    "pred = classifier.predict(val_X)\n",
    "print('val_X[10]',val_y[:10])\n",
    "print('pred[10]', pred[:10])\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.131)\n",
      "Epoch 0, loss: 2.302099\n",
      "Epoch 1, loss: 2.301690\n",
      "Epoch 2, loss: 2.302466\n",
      "Epoch 3, loss: 2.301759\n",
      "Epoch 4, loss: 2.302840\n",
      "Epoch 5, loss: 2.303159\n",
      "Epoch 6, loss: 2.301671\n",
      "Epoch 7, loss: 2.302135\n",
      "Epoch 8, loss: 2.300488\n",
      "Epoch 9, loss: 2.302301\n",
      "Epoch 10, loss: 2.302734\n",
      "Epoch 11, loss: 2.301382\n",
      "Epoch 12, loss: 2.301458\n",
      "Epoch 13, loss: 2.302540\n",
      "Epoch 14, loss: 2.302328\n",
      "Epoch 15, loss: 2.302900\n",
      "Epoch 16, loss: 2.301686\n",
      "Epoch 17, loss: 2.302472\n",
      "Epoch 18, loss: 2.301554\n",
      "Epoch 19, loss: 2.302166\n",
      "Epoch 20, loss: 2.301233\n",
      "Epoch 21, loss: 2.301720\n",
      "Epoch 22, loss: 2.302043\n",
      "Epoch 23, loss: 2.301621\n",
      "Epoch 24, loss: 2.301974\n",
      "Epoch 25, loss: 2.301781\n",
      "Epoch 26, loss: 2.302076\n",
      "Epoch 27, loss: 2.302434\n",
      "Epoch 28, loss: 2.302546\n",
      "Epoch 29, loss: 2.302168\n",
      "Epoch 30, loss: 2.301965\n",
      "Epoch 31, loss: 2.302220\n",
      "Epoch 32, loss: 2.302411\n",
      "Epoch 33, loss: 2.301733\n",
      "Epoch 34, loss: 2.301143\n",
      "Epoch 35, loss: 2.301641\n",
      "Epoch 36, loss: 2.301730\n",
      "Epoch 37, loss: 2.302544\n",
      "Epoch 38, loss: 2.300763\n",
      "Epoch 39, loss: 2.302629\n",
      "Epoch 40, loss: 2.302259\n",
      "Epoch 41, loss: 2.301080\n",
      "Epoch 42, loss: 2.302053\n",
      "Epoch 43, loss: 2.301254\n",
      "Epoch 44, loss: 2.301512\n",
      "Epoch 45, loss: 2.301858\n",
      "Epoch 46, loss: 2.301924\n",
      "Epoch 47, loss: 2.301727\n",
      "Epoch 48, loss: 2.302194\n",
      "Epoch 49, loss: 2.302028\n",
      "Epoch 50, loss: 2.301511\n",
      "Epoch 51, loss: 2.300784\n",
      "Epoch 52, loss: 2.301629\n",
      "Epoch 53, loss: 2.302083\n",
      "Epoch 54, loss: 2.302448\n",
      "Epoch 55, loss: 2.302197\n",
      "Epoch 56, loss: 2.301885\n",
      "Epoch 57, loss: 2.302293\n",
      "Epoch 58, loss: 2.301505\n",
      "Epoch 59, loss: 2.301089\n",
      "Epoch 60, loss: 2.302650\n",
      "Epoch 61, loss: 2.301271\n",
      "Epoch 62, loss: 2.301636\n",
      "Epoch 63, loss: 2.302732\n",
      "Epoch 64, loss: 2.302008\n",
      "Epoch 65, loss: 2.301584\n",
      "Epoch 66, loss: 2.301856\n",
      "Epoch 67, loss: 2.302409\n",
      "Epoch 68, loss: 2.301565\n",
      "Epoch 69, loss: 2.302309\n",
      "Epoch 70, loss: 2.302012\n",
      "Epoch 71, loss: 2.302335\n",
      "Epoch 72, loss: 2.301971\n",
      "Epoch 73, loss: 2.301713\n",
      "Epoch 74, loss: 2.301969\n",
      "Epoch 75, loss: 2.302407\n",
      "Epoch 76, loss: 2.302536\n",
      "Epoch 77, loss: 2.302216\n",
      "Epoch 78, loss: 2.301695\n",
      "Epoch 79, loss: 2.301756\n",
      "Epoch 80, loss: 2.302003\n",
      "Epoch 81, loss: 2.301916\n",
      "Epoch 82, loss: 2.301186\n",
      "Epoch 83, loss: 2.301454\n",
      "Epoch 84, loss: 2.301018\n",
      "Epoch 85, loss: 2.301000\n",
      "Epoch 86, loss: 2.301574\n",
      "Epoch 87, loss: 2.302814\n",
      "Epoch 88, loss: 2.301255\n",
      "Epoch 89, loss: 2.302457\n",
      "Epoch 90, loss: 2.302777\n",
      "Epoch 91, loss: 2.302266\n",
      "Epoch 92, loss: 2.301628\n",
      "Epoch 93, loss: 2.302196\n",
      "Epoch 94, loss: 2.301847\n",
      "Epoch 95, loss: 2.302112\n",
      "Epoch 96, loss: 2.301593\n",
      "Epoch 97, loss: 2.302066\n",
      "Epoch 98, loss: 2.301307\n",
      "Epoch 99, loss: 2.301733\n",
      "('Accuracy after training for 100 epochs: ', 0.124)\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
