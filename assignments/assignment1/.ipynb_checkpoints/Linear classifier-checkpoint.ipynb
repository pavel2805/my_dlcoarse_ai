{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_X.shape initial', (10000, 32, 32, 3))\n",
      "('train_x.mean', 0.446616470588233)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    print('train_x.mean',mean_image[0])\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    #print('train_flat_ones.shape', train_flat_with_ones.shape)\n",
    "    #print('train_flat_ones[0]', train_flat_with_ones[0])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "\n",
    "train_X_t, train_y_t, test_X, test_y = load_svhn(\"data\", max_train=11000, max_test=1000)\n",
    "\n",
    "train_X = train_X_t[0:10000]\n",
    "train_y =train_y_t[0:10000]\n",
    "test_X = train_X_t[10000:11000]\n",
    "test_y = train_y_t[10000:11000]    \n",
    "\n",
    "#train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000) \n",
    "\n",
    "print('train_X.shape initial', train_X.shape) #expect 1000,32,32,3\n",
    "#print('train_X[0,0]', train_X[0,0])\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_sum(x):\n",
    "    #print('x.shape',x.shape)\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (1,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2,))\n",
      "Gradient check passed!\n",
      "('it.shape=', (2, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "probs = linear_classifer.softmax(np.array([1, 1, 1]))\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0][0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "1\n",
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.zeros(5)\n",
    "print(a)\n",
    "print(np.ndim(a))\n",
    "b=a.reshape(1,-1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('probs', array([[0.57611688, 0.21194156, 0.21194156]]))\n",
      "('loss=', array([1.55144471]))\n"
     ]
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([1, 0, 0]))\n",
    "print('probs',probs)\n",
    "#linear_classifer.cross_entropy_loss(probs, 1)\n",
    "loss=linear_classifer.cross_entropy_loss(probs, np.array([[1]]))\n",
    "print('loss=',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss=', 1.551444713932051)\n",
      "('grad', array([ 0.57611688, -0.78805844,  0.21194156]))\n",
      "('it.shape=', (3,))\n",
      "Gradient check passed!\n",
      "----------star check batch------\n",
      "('loss=', 1.051444713932051)\n",
      "('grad', array([[ 0.28805844, -0.39402922,  0.10597078],\n",
      "       [-0.21194156,  0.10597078,  0.10597078]]))\n",
      "('it.shape=', (2, 3))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n",
    "\n",
    "print('----------star check batch------')\n",
    "predictions=np.array([[1,0,0],[1,0,0]],dtype=np.float)\n",
    "target_index=np.array([[1],[0]],dtype=np.int)\n",
    "loss, grad =linear_classifer.softmax_with_cross_entropy_batch(predictions, target_index)\n",
    "print('loss=',loss)\n",
    "print('grad',grad)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "#check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, np.array([1,0]), np.array([[1,0,0],[1,0,0]],np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- batch 2, num 4 ----------\n",
      "('predictions', array([[ 1.,  2., -1.,  1.],\n",
      "       [ 1.,  2., -1., -1.]]))\n",
      "('it.shape=', (2, 4))\n",
      "Gradient check passed!\n",
      "----------- batch 3, num 4 ----------\n",
      "('it.shape=', (3, 4))\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "\n",
    "print('----------- batch 2, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 2\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "print('predictions',predictions)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "print('----------- batch 3, num 4 ----------')\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy_batch(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss', 0.5472365148987376)\n",
      "('dW=', array([[-0.22019927,  0.22019927],\n",
      "       [-0.20332316,  0.20332316],\n",
      "       [ 0.23874859, -0.23874859]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 4\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss',loss)\n",
    "print('dW=',dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('target_index', array([1, 1, 1, 1, 1]))\n",
      "('loss SoftMax', 0.44141919750255204)\n",
      "('dW Softmax=', array([[-0.16896493,  0.16896493],\n",
      "       [-0.15906129,  0.15906129],\n",
      "       [ 0.19099887, -0.19099887]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('loss L2', 0.12)\n",
      "('dW  L2=', array([[ 0.02,  0.04],\n",
      "       [-0.02,  0.02],\n",
      "       [ 0.02,  0.04]]))\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n",
      "('it.shape=', (3, 2))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient, check_gradient_batch\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer\n",
    "\n",
    "# this is my test\n",
    "batch_size = 5\n",
    "num_features = 3\n",
    "\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "print('target_index',target_index)\n",
    "\n",
    "loss_sm, dW_sm = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('loss SoftMax',loss_sm)\n",
    "print('dW Softmax=',dW_sm)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "loss_l2, dW_l2=linear_classifer.l2_regularization(W, 0.01)\n",
    "print('loss L2',loss_l2)\n",
    "print('dW  L2=',dW_l2)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)\n",
    "\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, 0.01, X, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, 0.01,X,target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num_train', 9000)\n",
      "('num_features', 3073)\n",
      "('num_classe', 10)\n",
      "('batch_size', 20)\n",
      "('sections', array([  20,   40,   60,   80,  100,  120,  140,  160,  180,  200,  220,\n",
      "        240,  260,  280,  300,  320,  340,  360,  380,  400,  420,  440,\n",
      "        460,  480,  500,  520,  540,  560,  580,  600,  620,  640,  660,\n",
      "        680,  700,  720,  740,  760,  780,  800,  820,  840,  860,  880,\n",
      "        900,  920,  940,  960,  980, 1000, 1020, 1040, 1060, 1080, 1100,\n",
      "       1120, 1140, 1160, 1180, 1200, 1220, 1240, 1260, 1280, 1300, 1320,\n",
      "       1340, 1360, 1380, 1400, 1420, 1440, 1460, 1480, 1500, 1520, 1540,\n",
      "       1560, 1580, 1600, 1620, 1640, 1660, 1680, 1700, 1720, 1740, 1760,\n",
      "       1780, 1800, 1820, 1840, 1860, 1880, 1900, 1920, 1940, 1960, 1980,\n",
      "       2000, 2020, 2040, 2060, 2080, 2100, 2120, 2140, 2160, 2180, 2200,\n",
      "       2220, 2240, 2260, 2280, 2300, 2320, 2340, 2360, 2380, 2400, 2420,\n",
      "       2440, 2460, 2480, 2500, 2520, 2540, 2560, 2580, 2600, 2620, 2640,\n",
      "       2660, 2680, 2700, 2720, 2740, 2760, 2780, 2800, 2820, 2840, 2860,\n",
      "       2880, 2900, 2920, 2940, 2960, 2980, 3000, 3020, 3040, 3060, 3080,\n",
      "       3100, 3120, 3140, 3160, 3180, 3200, 3220, 3240, 3260, 3280, 3300,\n",
      "       3320, 3340, 3360, 3380, 3400, 3420, 3440, 3460, 3480, 3500, 3520,\n",
      "       3540, 3560, 3580, 3600, 3620, 3640, 3660, 3680, 3700, 3720, 3740,\n",
      "       3760, 3780, 3800, 3820, 3840, 3860, 3880, 3900, 3920, 3940, 3960,\n",
      "       3980, 4000, 4020, 4040, 4060, 4080, 4100, 4120, 4140, 4160, 4180,\n",
      "       4200, 4220, 4240, 4260, 4280, 4300, 4320, 4340, 4360, 4380, 4400,\n",
      "       4420, 4440, 4460, 4480, 4500, 4520, 4540, 4560, 4580, 4600, 4620,\n",
      "       4640, 4660, 4680, 4700, 4720, 4740, 4760, 4780, 4800, 4820, 4840,\n",
      "       4860, 4880, 4900, 4920, 4940, 4960, 4980, 5000, 5020, 5040, 5060,\n",
      "       5080, 5100, 5120, 5140, 5160, 5180, 5200, 5220, 5240, 5260, 5280,\n",
      "       5300, 5320, 5340, 5360, 5380, 5400, 5420, 5440, 5460, 5480, 5500,\n",
      "       5520, 5540, 5560, 5580, 5600, 5620, 5640, 5660, 5680, 5700, 5720,\n",
      "       5740, 5760, 5780, 5800, 5820, 5840, 5860, 5880, 5900, 5920, 5940,\n",
      "       5960, 5980, 6000, 6020, 6040, 6060, 6080, 6100, 6120, 6140, 6160,\n",
      "       6180, 6200, 6220, 6240, 6260, 6280, 6300, 6320, 6340, 6360, 6380,\n",
      "       6400, 6420, 6440, 6460, 6480, 6500, 6520, 6540, 6560, 6580, 6600,\n",
      "       6620, 6640, 6660, 6680, 6700, 6720, 6740, 6760, 6780, 6800, 6820,\n",
      "       6840, 6860, 6880, 6900, 6920, 6940, 6960, 6980, 7000, 7020, 7040,\n",
      "       7060, 7080, 7100, 7120, 7140, 7160, 7180, 7200, 7220, 7240, 7260,\n",
      "       7280, 7300, 7320, 7340, 7360, 7380, 7400, 7420, 7440, 7460, 7480,\n",
      "       7500, 7520, 7540, 7560, 7580, 7600, 7620, 7640, 7660, 7680, 7700,\n",
      "       7720, 7740, 7760, 7780, 7800, 7820, 7840, 7860, 7880, 7900, 7920,\n",
      "       7940, 7960, 7980, 8000, 8020, 8040, 8060, 8080, 8100, 8120, 8140,\n",
      "       8160, 8180, 8200, 8220, 8240, 8260, 8280, 8300, 8320, 8340, 8360,\n",
      "       8380, 8400, 8420, 8440, 8460, 8480, 8500, 8520, 8540, 8560, 8580,\n",
      "       8600, 8620, 8640, 8660, 8680, 8700, 8720, 8740, 8760, 8780, 8800,\n",
      "       8820, 8840, 8860, 8880, 8900, 8920, 8940, 8960, 8980]))\n",
      "('shuffled indices [0:10]', array([5957, 1550,  251, 4606, 4787, 4027, 3640, 6779, 8454, 1018]))\n",
      "('batches_indices', 450)\n",
      "('batches_indices[0].shape', (20,))\n",
      "('loss', 2.6095638566644457)\n",
      "('it.shape=', (3073, 10))\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "\n",
    "num_train=train_X.shape[0]\n",
    "num_features = train_X.shape[1]\n",
    "num_classes = np.max(train_y)+1\n",
    "batch_size=20\n",
    "reg=1e1\n",
    "learning_rate=1e-3\n",
    "print('num_train', num_train)\n",
    "print('num_features', num_features)\n",
    "print('num_classe',num_classes)\n",
    "print('batch_size',batch_size)\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "sections = np.arange(batch_size, num_train, batch_size)\n",
    "print('sections',sections)\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "print('shuffled indices [0:10]', shuffled_indices[0:10])\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n",
    "print('batches_indices', len(batches_indices))\n",
    "print('batches_indices[0].shape', batches_indices[0].shape)\n",
    "\n",
    "target_index=train_y[batches_indices[0]]\n",
    "X=train_X[batches_indices[0]]\n",
    "loss,dW=linear_classifer.linear_softmax_l2(W, reg, X, target_index)\n",
    "print('loss',loss)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax_l2(w, reg,X,target_index), W)\n",
    "#loss_history = classifier.fit(train_X, train_y, epochs=1, learning_rate=1e-3, batch_size=1, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1 3 5]\n",
      "[ 2  6 10]\n",
      "[5957 1550  251 4606 4787 4027 3640 6779 8454 1018 4551 5584 1874 8977\n",
      " 4632 6463 1626 5886 2762 5644]\n",
      "[1 1 4 4 1 1 1 2 1 6 3 9 4 2 5 1 8 6 1 3]\n"
     ]
    }
   ],
   "source": [
    "t=np.arange(10)*2\n",
    "i=np.array([1,3,5])\n",
    "print(t)\n",
    "print(i)\n",
    "print(t[i])\n",
    "print(batches_indices[0])\n",
    "print(train_y[batches_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first W = none\n",
      "nake W\n",
      "Epoch 0, loss: 2.397706\n",
      "Epoch 1, loss: 2.330696\n",
      "Epoch 2, loss: 2.310452\n",
      "Epoch 3, loss: 2.304400\n",
      "Epoch 4, loss: 2.303051\n",
      "Epoch 5, loss: 2.303128\n",
      "Epoch 6, loss: 2.302073\n",
      "Epoch 7, loss: 2.302643\n",
      "Epoch 8, loss: 2.302062\n",
      "Epoch 9, loss: 2.301643\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f074928e710>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val_X.shape', (1000, 3073))\n",
      "('val_X[10]', array([2, 3, 4, 3, 5, 5, 3, 6, 1, 7], dtype=uint8))\n",
      "('pred[10]', array([1, 1, 3, 1, 3, 2, 3, 3, 3, 4]))\n",
      "('Accuracy: ', 0.169)\n"
     ]
    }
   ],
   "source": [
    "print('val_X.shape',val_X.shape)\n",
    "pred = classifier.predict(val_X)\n",
    "print('val_X[10]',val_y[:10])\n",
    "print('pred[10]', pred[:10])\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.168)\n",
      "Epoch 0, loss: 2.301072\n",
      "Epoch 1, loss: 2.301391\n",
      "Epoch 2, loss: 2.302539\n",
      "Epoch 3, loss: 2.301473\n",
      "Epoch 4, loss: 2.300723\n",
      "Epoch 5, loss: 2.301839\n",
      "Epoch 6, loss: 2.302919\n",
      "Epoch 7, loss: 2.302318\n",
      "Epoch 8, loss: 2.302858\n",
      "Epoch 9, loss: 2.301858\n",
      "Epoch 10, loss: 2.301810\n",
      "Epoch 11, loss: 2.302400\n",
      "Epoch 12, loss: 2.301677\n",
      "Epoch 13, loss: 2.303707\n",
      "Epoch 14, loss: 2.301894\n",
      "Epoch 15, loss: 2.302791\n",
      "Epoch 16, loss: 2.301297\n",
      "Epoch 17, loss: 2.302651\n",
      "Epoch 18, loss: 2.302724\n",
      "Epoch 19, loss: 2.302103\n",
      "('Accuracy after training: ', 0.159)\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=20, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_X.shape', (9000, 3073))\n",
      "('train_y.shape', (9000,))\n",
      "('val_X.shape', (1000, 3073))\n",
      "('val_y.shape', (1000,))\n",
      "first W = none\n",
      "Epoch 0, loss: 2.396022\n",
      "Epoch 1, loss: 2.329750\n",
      "Epoch 2, loss: 2.310438\n",
      "Epoch 3, loss: 2.303819\n",
      "Epoch 4, loss: 2.302505\n",
      "Epoch 5, loss: 2.302268\n",
      "Epoch 6, loss: 2.302367\n",
      "Epoch 7, loss: 2.301751\n",
      "Epoch 8, loss: 2.302016\n",
      "Epoch 9, loss: 2.301596\n",
      "Epoch 10, loss: 2.302361\n",
      "Epoch 11, loss: 2.301883\n",
      "Epoch 12, loss: 2.301978\n",
      "Epoch 13, loss: 2.302454\n",
      "Epoch 14, loss: 2.302301\n",
      "Epoch 15, loss: 2.302720\n",
      "Epoch 16, loss: 2.301704\n",
      "Epoch 17, loss: 2.302396\n",
      "Epoch 18, loss: 2.302453\n",
      "Epoch 19, loss: 2.301481\n",
      "Epoch 20, loss: 2.301830\n",
      "Epoch 21, loss: 2.302437\n",
      "Epoch 22, loss: 2.302166\n",
      "Epoch 23, loss: 2.302593\n",
      "Epoch 24, loss: 2.301051\n",
      "Epoch 25, loss: 2.301876\n",
      "Epoch 26, loss: 2.301600\n",
      "Epoch 27, loss: 2.301571\n",
      "Epoch 28, loss: 2.301150\n",
      "Epoch 29, loss: 2.302963\n",
      "Epoch 30, loss: 2.302537\n",
      "Epoch 31, loss: 2.301754\n",
      "Epoch 32, loss: 2.302054\n",
      "Epoch 33, loss: 2.301162\n",
      "Epoch 34, loss: 2.301873\n",
      "Epoch 35, loss: 2.302246\n",
      "Epoch 36, loss: 2.302968\n",
      "Epoch 37, loss: 2.302085\n",
      "Epoch 38, loss: 2.302343\n",
      "Epoch 39, loss: 2.302014\n",
      "Epoch 40, loss: 2.302510\n",
      "Epoch 41, loss: 2.302029\n",
      "Epoch 42, loss: 2.301997\n",
      "Epoch 43, loss: 2.301989\n",
      "Epoch 44, loss: 2.302068\n",
      "Epoch 45, loss: 2.303432\n",
      "Epoch 46, loss: 2.302472\n",
      "Epoch 47, loss: 2.303269\n",
      "Epoch 48, loss: 2.300724\n",
      "Epoch 49, loss: 2.301767\n",
      "('learning rate =', 0.001, ' Accuracy', 0.159)\n",
      "Epoch 0, loss: 2.572095\n",
      "Epoch 1, loss: 2.541844\n",
      "Epoch 2, loss: 2.515199\n",
      "Epoch 3, loss: 2.490371\n",
      "Epoch 4, loss: 2.468964\n",
      "Epoch 5, loss: 2.451121\n",
      "Epoch 6, loss: 2.433552\n",
      "Epoch 7, loss: 2.418391\n",
      "Epoch 8, loss: 2.405152\n",
      "Epoch 9, loss: 2.392832\n",
      "Epoch 10, loss: 2.382571\n",
      "Epoch 11, loss: 2.374611\n",
      "Epoch 12, loss: 2.366075\n",
      "Epoch 13, loss: 2.358548\n",
      "Epoch 14, loss: 2.352050\n",
      "Epoch 15, loss: 2.346741\n",
      "Epoch 16, loss: 2.341285\n",
      "Epoch 17, loss: 2.337090\n",
      "Epoch 18, loss: 2.332831\n",
      "Epoch 19, loss: 2.328682\n",
      "Epoch 20, loss: 2.327030\n",
      "Epoch 21, loss: 2.323363\n",
      "Epoch 22, loss: 2.321175\n",
      "Epoch 23, loss: 2.319390\n",
      "Epoch 24, loss: 2.317609\n",
      "Epoch 25, loss: 2.315201\n",
      "Epoch 26, loss: 2.312355\n",
      "Epoch 27, loss: 2.312652\n",
      "Epoch 28, loss: 2.311645\n",
      "Epoch 29, loss: 2.310088\n",
      "Epoch 30, loss: 2.309862\n",
      "Epoch 31, loss: 2.308199\n",
      "Epoch 32, loss: 2.308363\n",
      "Epoch 33, loss: 2.307007\n",
      "Epoch 34, loss: 2.307130\n",
      "Epoch 35, loss: 2.305532\n",
      "Epoch 36, loss: 2.306145\n",
      "Epoch 37, loss: 2.305179\n",
      "Epoch 38, loss: 2.304491\n",
      "Epoch 39, loss: 2.304533\n",
      "Epoch 40, loss: 2.304271\n",
      "Epoch 41, loss: 2.304368\n",
      "Epoch 42, loss: 2.303002\n",
      "Epoch 43, loss: 2.302726\n",
      "Epoch 44, loss: 2.302442\n",
      "Epoch 45, loss: 2.302657\n",
      "Epoch 46, loss: 2.303682\n",
      "Epoch 47, loss: 2.303384\n",
      "Epoch 48, loss: 2.302093\n",
      "Epoch 49, loss: 2.303498\n",
      "('learning rate =', 0.0001, ' Accuracy', 0.159)\n",
      "Epoch 0, loss: 2.603135\n",
      "Epoch 1, loss: 2.597999\n",
      "Epoch 2, loss: 2.594852\n",
      "Epoch 3, loss: 2.592301\n",
      "Epoch 4, loss: 2.588544\n",
      "Epoch 5, loss: 2.585174\n",
      "Epoch 6, loss: 2.581271\n",
      "Epoch 7, loss: 2.578484\n",
      "Epoch 8, loss: 2.574497\n",
      "Epoch 9, loss: 2.570973\n",
      "Epoch 10, loss: 2.569176\n",
      "Epoch 11, loss: 2.564918\n",
      "Epoch 12, loss: 2.562345\n",
      "Epoch 13, loss: 2.558751\n",
      "Epoch 14, loss: 2.554967\n",
      "Epoch 15, loss: 2.553020\n",
      "Epoch 16, loss: 2.550089\n",
      "Epoch 17, loss: 2.548045\n",
      "Epoch 18, loss: 2.544602\n",
      "Epoch 19, loss: 2.541452\n",
      "Epoch 20, loss: 2.538691\n",
      "Epoch 21, loss: 2.535890\n",
      "Epoch 22, loss: 2.533134\n",
      "Epoch 23, loss: 2.530286\n",
      "Epoch 24, loss: 2.528071\n",
      "Epoch 25, loss: 2.524936\n",
      "Epoch 26, loss: 2.522012\n",
      "Epoch 27, loss: 2.519268\n",
      "Epoch 28, loss: 2.516879\n",
      "Epoch 29, loss: 2.513850\n",
      "Epoch 30, loss: 2.511643\n",
      "Epoch 31, loss: 2.508588\n",
      "Epoch 32, loss: 2.507056\n",
      "Epoch 33, loss: 2.503629\n",
      "Epoch 34, loss: 2.501782\n",
      "Epoch 35, loss: 2.499527\n",
      "Epoch 36, loss: 2.496640\n",
      "Epoch 37, loss: 2.495569\n",
      "Epoch 38, loss: 2.492169\n",
      "Epoch 39, loss: 2.489191\n",
      "Epoch 40, loss: 2.487326\n",
      "Epoch 41, loss: 2.485572\n",
      "Epoch 42, loss: 2.483917\n",
      "Epoch 43, loss: 2.481425\n",
      "Epoch 44, loss: 2.479492\n",
      "Epoch 45, loss: 2.477223\n",
      "Epoch 46, loss: 2.474249\n",
      "Epoch 47, loss: 2.473491\n",
      "Epoch 48, loss: 2.470358\n",
      "Epoch 49, loss: 2.470059\n",
      "('learning rate =', 1e-05, ' Accuracy', 0.159)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "print('train_X.shape', train_X.shape)\n",
    "print('train_y.shape',train_y.shape)\n",
    "print('val_X.shape',val_X.shape)\n",
    "print('val_y.shape',val_y.shape)\n",
    "\n",
    "W= 0.001 * np.random.randn(num_features, num_classes)\n",
    "classifier1 = linear_classifer.LinearSoftmaxClassifier()\n",
    "classifier1.W=W\n",
    "\n",
    "for lr in learning_rates:\n",
    "    classifier1.W=W\n",
    "    loss_history=classifier1.fit(train_X, train_y, epochs=50, learning_rate=lr, batch_size=300, reg=1e1)\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n",
    "    print('learning rate =', lr,  ' Accuracy',   accuracy)\n",
    "\n",
    "for rs in reg_strengths:\n",
    "    classifier1.W=W\n",
    "    loss_history=classifier1.fit(train_X, train_y, epochs=50, learning_rate=1e-3, batch_size=300, reg=rs)\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n",
    "    print('learning rate =', lr,  ' Accuracy',   accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_X.shape', (9000, 3073))\n",
      "('train_y.shape', (9000,))\n",
      "('val_X.shape', (1000, 3073))\n",
      "('val_y.shape', (1000,))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5edcb1928827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclassifier1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSoftmaxClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "print('train_X.shape', train_X.shape)\n",
    "print('train_y.shape',train_y.shape)\n",
    "print('val_X.shape',val_X.shape)\n",
    "print('val_y.shape',val_y.shape)\n",
    "\n",
    "W= 0.001 * np.random.randn(num_features, num_classes)\n",
    "classifier1 = linear_classifer.LinearSoftmaxClassifier(W)\n",
    "loss_history = classifier1.fit(train_X, train_y, epochs=50, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    classifier.fit(train_X, train_y, epochs=2, learning_rate=lr, batch_size=300, reg=1e1)\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    #print(\"learning rate = %d Accuracy %d \" %(lr, accuracy))\n",
    "    print('learning rate =', lr,  ' Accuracy',   accuracy)\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
